{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import glob\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yolo, GRU 동시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size:  25\n",
      "raw_test_list_len:  9064\n",
      "test_list len:  8232\n",
      "test_dict length:  ['N: 5', 'BY: 5', 'FY: 5', 'SY: 5']\n",
      "0 01421_O_B_N_C1.mp4\n",
      "No bounding boxes detected.\n",
      "1 00798_O_E_N_C6.mp4\n",
      "Transformed coordinates: (1411.6376037597656, 956.7185668945312, 1925.23388671875, 1457.7900924682617)\n",
      "Transformed coordinates: (1411.2158203125, 957.1740188598633, 1925.0709228515625, 1457.7629013061523)\n",
      "Transformed coordinates: (1410.7880859375, 957.5508842468262, 1925.0592041015625, 1457.728603363037)\n",
      "Transformed coordinates: (1410.1762390136719, 957.5812683105469, 1925.2078857421875, 1457.8096618652344)\n",
      "Transformed coordinates: (1409.5651245117188, 957.6317367553711, 1925.4929809570312, 1457.8362350463867)\n",
      "Transformed coordinates: (1408.9421081542969, 957.3811454772949, 1925.6444091796875, 1457.979091644287)\n",
      "Transformed coordinates: (1408.6835632324219, 957.533374786377, 1925.5294189453125, 1457.950252532959)\n",
      "Transformed coordinates: (1408.6603088378906, 956.9822387695312, 1925.256591796875, 1457.9740447998047)\n",
      "Transformed coordinates: (1408.5346069335938, 956.674690246582, 1924.5287475585938, 1457.9178085327148)\n",
      "Transformed coordinates: (1408.1385498046875, 956.7790260314941, 1924.5758056640625, 1457.9269752502441)\n",
      "Transformed coordinates: (1407.5848388671875, 956.6086692810059, 1924.5391845703125, 1457.8822746276855)\n",
      "Transformed coordinates: (1407.3700561523438, 956.1287040710449, 1924.1284790039062, 1457.9533424377441)\n",
      "Transformed coordinates: (1407.123046875, 955.4500579833984, 1923.381591796875, 1457.9410858154297)\n",
      "Transformed coordinates: (1406.7702026367188, 954.8963470458984, 1922.7456665039062, 1457.9917602539062)\n",
      "Transformed coordinates: (1406.1478271484375, 954.3216247558594, 1922.700439453125, 1458.0220413208008)\n",
      "Transformed coordinates: (1406.3300170898438, 954.1873168945312, 1922.2827758789062, 1457.8768157958984)\n",
      "Transformed coordinates: (1406.4296264648438, 954.097297668457, 1921.4708862304688, 1457.7534255981445)\n",
      "Transformed coordinates: (1406.3611450195312, 954.0981216430664, 1919.9606323242188, 1457.9563293457031)\n",
      "Transformed coordinates: (1405.9997863769531, 954.2472610473633, 1918.9229736328125, 1458.1897201538086)\n",
      "Transformed coordinates: (1406.1454467773438, 954.4439849853516, 1917.3843383789062, 1458.158821105957)\n",
      "Transformed coordinates: (1406.0797119140625, 954.498779296875, 1917.35009765625, 1458.0564422607422)\n",
      "Transformed coordinates: (1406.14892578125, 954.5092849731445, 1916.7513427734375, 1458.0755996704102)\n",
      "Transformed coordinates: (1406.0694580078125, 954.4666442871094, 1917.0325927734375, 1458.0906372070312)\n",
      "Transformed coordinates: (1405.9844055175781, 954.5263824462891, 1917.1585693359375, 1458.243896484375)\n",
      "Transformed coordinates: (1405.4637451171875, 954.151782989502, 1917.44091796875, 1458.0172004699707)\n",
      "Transformed coordinates: (1405.4593505859375, 954.2798080444336, 1917.44091796875, 1458.1421356201172)\n",
      "Transformed coordinates: (1405.8292236328125, 954.7949981689453, 1917.488525390625, 1458.1013488769531)\n",
      "Transformed coordinates: (1406.02001953125, 954.6994171142578, 1917.533935546875, 1458.0469665527344)\n",
      "Transformed coordinates: (1406.03466796875, 954.6646041870117, 1917.4903564453125, 1457.9116287231445)\n",
      "Transformed coordinates: (1406.1415100097656, 954.6491546630859, 1918.119140625, 1458.0098876953125)\n",
      "Transformed coordinates: (1406.1242065429688, 954.5679931640625, 1919.6167602539062, 1458.1075286865234)\n",
      "Transformed coordinates: (1406.6076049804688, 954.2666244506836, 1921.8506469726562, 1457.9470596313477)\n",
      "Transformed coordinates: (1406.7567443847656, 954.0081024169922, 1924.37255859375, 1457.7035751342773)\n",
      "Transformed coordinates: (1407.1261596679688, 954.0779342651367, 1924.5759887695312, 1457.6545486450195)\n",
      "Transformed coordinates: (1407.5001525878906, 954.0177841186523, 1925.0390625, 1457.714698791504)\n",
      "Transformed coordinates: (1407.6412353515625, 953.5150566101074, 1925.195068359375, 1457.79390335083)\n",
      "Transformed coordinates: (1407.7763671875, 953.2253265380859, 1925.1943359375, 1457.7880325317383)\n",
      "Transformed coordinates: (1408.3229370117188, 952.7890319824219, 1925.8558959960938, 1457.8224334716797)\n",
      "Transformed coordinates: (1408.9928283691406, 952.2768287658691, 1926.56103515625, 1457.8443717956543)\n",
      "Transformed coordinates: (1409.7680053710938, 951.9338493347168, 1927.1753540039062, 1457.8866004943848)\n",
      "Transformed coordinates: (1410.791015625, 951.7130241394043, 1928.025146484375, 1457.751262664795)\n",
      "Transformed coordinates: (1411.24658203125, 951.7508239746094, 1928.3624267578125, 1457.7151107788086)\n",
      "Transformed coordinates: (1411.3143310546875, 951.5526580810547, 1928.427978515625, 1457.6831817626953)\n",
      "Transformed coordinates: (1411.897705078125, 951.7819290161133, 1926.897216796875, 1457.822021484375)\n",
      "Transformed coordinates: (1412.3578491210938, 951.6982955932617, 1926.8684692382812, 1457.8626022338867)\n",
      "Transformed coordinates: (1412.3446655273438, 951.5229949951172, 1926.6047973632812, 1457.980224609375)\n",
      "Transformed coordinates: (1412.2373657226562, 951.0277862548828, 1926.9896850585938, 1457.8111038208008)\n",
      "Transformed coordinates: (1412.6329650878906, 950.73486328125, 1928.62060546875, 1457.8117218017578)\n",
      "Transformed coordinates: (1413.1410827636719, 951.0533294677734, 1930.5208740234375, 1457.836441040039)\n",
      "Transformed coordinates: (1413.8358764648438, 951.7481460571289, 1932.6403198242188, 1457.7964782714844)\n",
      "Transformed coordinates: (1414.8677673339844, 951.2063827514648, 1933.220947265625, 1457.7610473632812)\n",
      "Transformed coordinates: (1415.2941284179688, 950.8734970092773, 1932.8567504882812, 1457.8261413574219)\n",
      "Transformed coordinates: (1416.3870849609375, 950.9087219238281, 1931.9110107421875, 1458.0170974731445)\n",
      "Transformed coordinates: (1416.1789855957031, 952.2888793945312, 1932.1043701171875, 1457.9575653076172)\n",
      "Transformed coordinates: (1417.661865234375, 952.9841079711914, 1935.1453857421875, 1458.0558242797852)\n",
      "Transformed coordinates: (1419.4398193359375, 952.481071472168, 1936.6102294921875, 1458.1229782104492)\n",
      "Transformed coordinates: (1421.1656799316406, 952.4225692749023, 1937.98388671875, 1458.2539901733398)\n",
      "Transformed coordinates: (1422.7451477050781, 952.5474014282227, 1939.2264404296875, 1458.0751876831055)\n",
      "Transformed coordinates: (1421.8294372558594, 952.9311676025391, 1939.84423828125, 1458.375732421875)\n",
      "Transformed coordinates: (1422.5183715820312, 951.6629676818848, 1941.1638793945312, 1458.5569038391113)\n",
      "Transformed coordinates: (1423.7297973632812, 950.5554428100586, 1942.1508178710938, 1458.6064453125)\n",
      "Transformed coordinates: (1425.7432250976562, 949.8334350585938, 1944.0891723632812, 1458.585433959961)\n",
      "Transformed coordinates: (1422.9688110351562, 949.6998481750488, 1945.0446166992188, 1458.5342445373535)\n",
      "Transformed coordinates: (1429.5338745117188, 948.4711990356445, 1946.4105834960938, 1458.5491790771484)\n",
      "Transformed coordinates: (1432.313232421875, 947.0411911010742, 1948.0869140625, 1458.4323806762695)\n",
      "Transformed coordinates: (1431.8206787109375, 945.9248085021973, 1951.102294921875, 1458.4024085998535)\n",
      "Transformed coordinates: (1435.8261108398438, 945.8229446411133, 1952.8994750976562, 1458.3318557739258)\n",
      "Transformed coordinates: (1437.6282348632812, 944.1782913208008, 1953.9717407226562, 1458.318260192871)\n",
      "Transformed coordinates: (1439.3446655273438, 943.4072570800781, 1955.7694702148438, 1458.4216690063477)\n",
      "Transformed coordinates: (1439.0935363769531, 942.2429809570312, 1957.3172607421875, 1458.2428665161133)\n",
      "Transformed coordinates: (1441.031982421875, 940.2672958374023, 1958.2152099609375, 1458.2712936401367)\n",
      "Transformed coordinates: (1440.306884765625, 939.4286956787109, 1959.0252685546875, 1458.2480163574219)\n",
      "Transformed coordinates: (1440.7880859375, 938.2046813964844, 1960.0513916015625, 1458.341537475586)\n",
      "Transformed coordinates: (1439.2840576171875, 936.7985687255859, 1961.318115234375, 1458.6286926269531)\n",
      "Transformed coordinates: (1438.5098876953125, 936.0617294311523, 1962.2724609375, 1458.6037673950195)\n",
      "Transformed coordinates: (1439.3675537109375, 934.5425262451172, 1963.481689453125, 1458.7541427612305)\n",
      "Transformed coordinates: (1439.5014038085938, 933.3742332458496, 1963.4598999023438, 1458.793384552002)\n",
      "Transformed coordinates: (1441.5151977539062, 933.134765625, 1965.7758178710938, 1458.8989562988281)\n",
      "Transformed coordinates: (1446.1901550292969, 931.0268325805664, 1969.4337158203125, 1458.9106979370117)\n",
      "Transformed coordinates: (1448.015625, 929.219856262207, 1972.03271484375, 1459.0153427124023)\n",
      "Transformed coordinates: (1455.8382568359375, 928.3347015380859, 1974.9063720703125, 1459.0851745605469)\n",
      "Transformed coordinates: (1460.0899658203125, 927.2503509521484, 1978.7252197265625, 1459.1300811767578)\n",
      "Transformed coordinates: (1462.7766723632812, 925.5822143554688, 1981.3555297851562, 1459.027084350586)\n",
      "Transformed coordinates: (1466.0419006347656, 925.0169677734375, 1983.211669921875, 1459.003189086914)\n",
      "Transformed coordinates: (1464.3012084960938, 924.4022827148438, 1985.3256225585938, 1458.9207916259766)\n",
      "Transformed coordinates: (1464.9699096679688, 923.0585861206055, 1986.3854370117188, 1458.8925704956055)\n",
      "Transformed coordinates: (1465.1826782226562, 921.0620956420898, 1988.2282104492188, 1458.0154495239258)\n",
      "Transformed coordinates: (1468.1284790039062, 920.9292297363281, 1989.3685913085938, 1458.8519897460938)\n",
      "Transformed coordinates: (1470.8204956054688, 919.2433776855469, 1990.6287231445312, 1457.8092498779297)\n",
      "Transformed coordinates: (1473.0326843261719, 918.6118011474609, 1993.5926513671875, 1457.8733139038086)\n",
      "Transformed coordinates: (1474.0482788085938, 917.2192840576172, 1994.5087280273438, 1457.8632202148438)\n",
      "Transformed coordinates: (1478.445556640625, 914.5318908691406, 1995.19189453125, 1457.8096618652344)\n",
      "Transformed coordinates: (1480.6411743164062, 913.4821472167969, 1997.7035522460938, 1458.0041198730469)\n",
      "Transformed coordinates: (1480.7883911132812, 912.1846961975098, 2002.9974975585938, 1458.29838180542)\n",
      "Transformed coordinates: (1481.7759704589844, 911.3989334106445, 2005.27001953125, 1458.7450790405273)\n",
      "Transformed coordinates: (1486.388671875, 909.7491302490234, 2005.07958984375, 1458.3468933105469)\n",
      "Transformed coordinates: (1488.9817199707031, 908.1349639892578, 2005.2740478515625, 1458.3771743774414)\n",
      "[1907.1873092651367, 1129.550485610962, 1829.0209579467773, 1040.4091787338257, 1824.9593353271484, 1044.0481209754944, 1745.614356994629, 1232.443413734436, 1730.7188415527344, 1243.450813293457, 1585.72265625, 1023.9362525939941, 1571.513671875, 1024.6529817581177, 1685.3015899658203, 1224.4609022140503, 1660.5517959594727, 1221.2977409362793, 1614.6648788452148, 1388.8278722763062, 1612.9918670654297, 1409.8802947998047, 0.938300112447983, 0, 5.422930657796059]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     248.16      269.08      334.21      432.11]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.88511]\n",
      "Transformed coordinates: (1488.6642150878906, 907.4842300415039, 2007.410888671875, 1458.297248840332)\n",
      "[1907.713623046875, 1127.3355388641357, 1828.8581085205078, 1040.456428527832, 1824.9540710449219, 1041.73082113266, 1752.4790954589844, 1223.0298900604248, 1738.5783004760742, 1226.9983577728271, 1587.6902389526367, 1023.5915994644165, 1571.5160751342773, 1023.8758063316345, 1694.1535949707031, 1221.5812397003174, 1665.6477355957031, 1217.0388221740723, 1615.9161758422852, 1384.1036653518677, 1612.9742431640625, 1406.76438331604, 0.9417836105530483, 0, 1.4997593714215187]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     248.11      268.88      334.57      432.09]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.88701]\n",
      "Transformed coordinates: (1489.9788208007812, 905.6267852783203, 2009.0714721679688, 1458.4976806640625)\n",
      "[1908.0452728271484, 1122.670555114746, 1829.1150283813477, 1037.8636765480042, 1824.9473190307617, 1040.8817410469055, 1759.1138076782227, 1218.0601644515991, 1740.1850509643555, 1220.9460067749023, 1589.0095138549805, 1019.4618129730225, 1571.4383697509766, 1022.8619956970215, 1693.0586242675781, 1212.8002452850342, 1662.9285049438477, 1210.8575963974, 1620.4337310791016, 1375.8767938613892, 1614.729995727539, 1400.3051948547363, 0.9389039207878951, 0, 2.7092224060484025]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     248.33      268.33      334.85      432.15]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.88866]\n",
      "Transformed coordinates: (1494.2681579589844, 904.8866500854492, 2011.077392578125, 1458.7745361328125)\n",
      "[1908.321647644043, 1118.4115076065063, 1829.1093063354492, 1033.4998941421509, 1824.9964141845703, 1040.1971340179443, 1761.4516067504883, 1204.0701055526733, 1745.5460357666016, 1209.286379814148, 1589.512939453125, 1015.290699005127, 1572.835350036621, 1021.7134523391724, 1685.397834777832, 1212.8776216506958, 1654.782485961914, 1212.7199077606201, 1619.996452331543, 1377.276520729065, 1612.4212646484375, 1400.8147716522217, 0.9330574790273495, 0, 3.1043092125543272]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     249.04      268.11      335.18      432.23]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.88107]\n",
      "Transformed coordinates: (1495.705078125, 903.3352088928223, 2013.064453125, 1458.9482917785645)\n",
      "[1910.5137634277344, 1115.9462785720825, 1832.369384765625, 1033.0715560913086, 1825.0498580932617, 1038.300061225891, 1764.6287155151367, 1200.118374824524, 1739.0425872802734, 1209.4341802597046, 1598.888168334961, 1013.6842703819275, 1572.8348922729492, 1016.2759923934937, 1684.500503540039, 1214.0680074691772, 1649.5338821411133, 1213.3256578445435, 1617.9797744750977, 1377.8499555587769, 1611.3294982910156, 1401.4194917678833, 0.9311504551205667, 0, 2.432700410169113]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     249.28      267.65      335.51      432.28]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.88033]\n",
      "Transformed coordinates: (1497.1028137207031, 900.3189468383789, 2015.6905517578125, 1459.0174026489258)\n",
      "[1911.2763977050781, 1115.5214166641235, 1832.3014068603516, 1034.3862462043762, 1827.4128341674805, 1032.7783370018005, 1764.7330856323242, 1193.550009727478, 1738.960189819336, 1177.9206275939941, 1599.6255111694336, 1014.1230368614197, 1574.4073104858398, 1011.0410499572754, 1681.6254043579102, 1215.3555965423584, 1644.429931640625, 1212.3344421386719, 1621.1003494262695, 1384.092721939087, 1609.738540649414, 1401.304907798767, 0.9282068576415774, 0, 1.8500323392747107]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     249.52      266.76      335.95       432.3]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.89073]\n",
      "Transformed coordinates: (1498.0545043945312, 897.9715461730957, 2018.9104614257812, 1458.9944343566895)\n",
      "[1918.022689819336, 1115.4630947113037, 1833.6053466796875, 1034.4873118400574, 1827.1485900878906, 1026.7822480201721, 1769.7404479980469, 1191.9655323028564, 1736.27197265625, 1183.6159658432007, 1604.1017532348633, 1011.7553329467773, 1574.3976974487305, 1006.2979817390442, 1686.2343978881836, 1202.5851488113403, 1646.206398010254, 1202.5520610809326, 1623.4344863891602, 1379.5016384124756, 1612.448844909668, 1398.9661073684692, 0.9284041132753229, 0, 3.2670735649481744]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     249.68      266.07      336.49      432.29]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.89497]\n",
      "Transformed coordinates: (1498.903564453125, 896.9116058349609, 2024.2591552734375, 1459.6718444824219)\n",
      "[1920.6275939941406, 1114.1228485107422, 1833.5417175292969, 1034.6312499046326, 1827.249870300293, 1022.84912109375, 1778.408203125, 1197.308235168457, 1751.7352294921875, 1182.9919338226318, 1605.5033111572266, 1009.1473889350891, 1576.1711883544922, 999.40105676651, 1672.130126953125, 1205.8086919784546, 1641.0796737670898, 1202.5883674621582, 1631.3154602050781, 1379.6975898742676, 1613.6804580688477, 1395.134882926941, 0.9335335987541571, 0, 1.9233709481905565]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     249.82      265.75      337.38       432.5]]\n",
      "Classes: [          0]\n",
      "Confidences: [     0.8968]\n",
      "Transformed coordinates: (1500.8875122070312, 895.5957183837891, 2025.0961303710938, 1459.0967102050781)\n",
      "[1920.3096771240234, 1111.334981918335, 1834.149055480957, 1036.6165781021118, 1829.541893005371, 1018.8635945320129, 1784.9919891357422, 1199.9864101409912, 1749.1259765625, 1166.4644193649292, 1607.3993682861328, 1009.2637753486633, 1578.621940612793, 994.1047668457031, 1672.778434753418, 1206.7383670806885, 1640.482177734375, 1201.0857725143433, 1635.256004333496, 1380.0435304641724, 1613.766860961914, 1388.2819890975952, 0.9302709769325697, 0, 1.8132052054221073]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     250.15      265.36      337.52      432.32]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.90806]\n",
      "Transformed coordinates: (1501.4226379394531, 894.9776344299316, 2028.2728271484375, 1458.7033653259277)\n",
      "[1921.2217712402344, 1109.1477155685425, 1835.6789016723633, 1038.4569382667542, 1829.505386352539, 1018.5542821884155, 1800.0156784057617, 1212.5751972198486, 1747.0741653442383, 1160.1753044128418, 1610.3203582763672, 1008.9098525047302, 1578.706398010254, 995.3789019584656, 1679.8377227783203, 1204.8018980026245, 1642.3332595825195, 1196.8733310699463, 1657.7402114868164, 1377.9125261306763, 1626.3567352294922, 1380.6836557388306, 0.9345860235465906, 0, 0.831112629420792]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     250.24      265.18      338.05      432.21]]\n",
      "Classes: [          0]\n",
      "Confidences: [     0.8968]\n",
      "Transformed coordinates: (1503.8587646484375, 894.3607864379883, 2028.350830078125, 1459.3696517944336)\n",
      "[1926.6973114013672, 1109.5188903808594, 1836.6077041625977, 1038.7641906738281, 1830.9497451782227, 1018.8011527061462, 1802.5897979736328, 1215.182433128357, 1748.8776397705078, 1158.4014415740967, 1610.1397705078125, 1006.4437866210938, 1581.6443252563477, 993.9390707015991, 1679.8453903198242, 1205.5094861984253, 1641.9643020629883, 1198.4181547164917, 1667.9712295532227, 1380.2645874023438, 1633.143539428711, 1383.6950254440308, 0.9282899748817267, 0, 2.634351929121203]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     250.64         265      338.06      432.41]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.89503]\n",
      "Transformed coordinates: (1507.82080078125, 895.5443229675293, 2030.46826171875, 1459.0304832458496)\n",
      "[1936.6561889648438, 1110.2124452590942, 1840.861701965332, 1038.7753915786743, 1832.1178436279297, 1015.811026096344, 1801.9026947021484, 1213.7839937210083, 1748.6749649047852, 1151.8423461914062, 1614.3592071533203, 1005.8473062515259, 1586.1602783203125, 991.649580001831, 1687.595443725586, 1203.3787393569946, 1643.0744934082031, 1196.1920070648193, 1661.6439056396484, 1380.0880765914917, 1632.0549774169922, 1382.1460819244385, 0.927524929235797, 0, 5.183277334548335]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[      251.3      265.35      338.41      432.31]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.91539]\n",
      "Transformed coordinates: (1509.9754028320312, 896.1370697021484, 2031.3898315429688, 1458.745491027832)\n",
      "[1934.4092559814453, 1108.6506271362305, 1842.4798965454102, 1037.2764658927917, 1834.397850036621, 1009.6249723434448, 1812.718391418457, 1227.6185274124146, 1759.8197937011719, 1156.279320716858, 1615.301742553711, 1005.5790638923645, 1589.7978973388672, 990.4011940956116, 1708.3784866333008, 1199.2811393737793, 1654.2899322509766, 1188.870348930359, 1672.979736328125, 1370.2208518981934, 1631.711082458496, 1375.9971714019775, 0.926780348367911, 0, 3.1310266539838465]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     251.66      265.52      338.56      432.22]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.92118]\n",
      "Transformed coordinates: (1512.7402954101562, 897.2709617614746, 2038.5525512695312, 1458.8113059997559)\n",
      "[1936.0791778564453, 1107.6823282241821, 1842.626953125, 1039.9330759048462, 1838.3396530151367, 1003.7077403068542, 1818.5481262207031, 1242.7012538909912, 1764.2629623413086, 1149.363341331482, 1614.2316055297852, 1006.4502882957458, 1590.3761672973633, 991.781222820282, 1715.6330108642578, 1194.877896308899, 1675.7059478759766, 1187.8425693511963, 1675.1420974731445, 1370.490574836731, 1649.351806640625, 1375.502529144287, 0.9363748504528723, 0, 2.381580611511858]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     252.12      265.86      339.76      432.24]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.91741]\n",
      "Transformed coordinates: (1514.7693786621094, 898.0822677612305, 2038.26416015625, 1458.2144393920898)\n",
      "[1937.3854064941406, 1107.6915979385376, 1844.7668838500977, 1040.10018825531, 1839.056625366211, 1002.598979473114, 1832.6745986938477, 1262.83842086792, 1770.8811950683594, 1150.6298160552979, 1620.4082107543945, 1011.3975477218628, 1594.423484802246, 992.3186087608337, 1729.4029998779297, 1196.0771656036377, 1680.9490585327148, 1186.1160850524902, 1670.2483749389648, 1377.9148435592651, 1638.5036087036133, 1377.3443698883057, 0.9345915267997431, 0, 1.4220875045642973]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     252.46       266.1      339.71      432.06]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.91619]\n",
      "Transformed coordinates: (1518.0303955078125, 898.9661865234375, 2046.638671875, 1458.1705627441406)\n",
      "[1942.3271942138672, 1106.04660987854, 1846.0815811157227, 1037.8298163414001, 1842.1992874145508, 1004.5527005195618, 1833.9613723754883, 1254.8941469192505, 1768.0321884155273, 1153.2982063293457, 1619.6538162231445, 1012.8084111213684, 1594.3572235107422, 998.7322854995728, 1736.5342712402344, 1194.1806077957153, 1694.0747451782227, 1187.632327079773, 1672.3436737060547, 1375.2729749679565, 1637.5396728515625, 1382.7012348175049, 0.9452863726491293, 0, 3.200555372686213]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     253.01      266.36      341.11      432.05]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.92022]\n",
      "Transformed coordinates: (1520.8650512695312, 899.568717956543, 2047.2496948242188, 1457.9627151489258)\n",
      "[1945.7109832763672, 1104.0010929107666, 1847.9578399658203, 1037.5816583633423, 1844.9863815307617, 1005.1755738258362, 1844.6098709106445, 1257.3566722869873, 1740.256118774414, 1176.967134475708, 1621.9072723388672, 1014.1734409332275, 1596.2190628051758, 1004.223883152008, 1740.6763458251953, 1195.5809783935547, 1695.230941772461, 1190.6881141662598, 1683.9982223510742, 1374.422607421875, 1636.1896133422852, 1387.8348541259766, 0.9426760427249594, 0, 2.73958782282144]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     253.48      266.54      341.21      431.99]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.92225]\n",
      "Transformed coordinates: (1522.7076416015625, 900.1996765136719, 2043.891357421875, 1458.5281677246094)\n",
      "[1947.9810333251953, 1103.502459526062, 1851.9050216674805, 1037.8713369369507, 1847.3614883422852, 1004.0278029441833, 1843.037452697754, 1252.6615619659424, 1772.8622817993164, 1165.0885105133057, 1632.414665222168, 1017.2181558609009, 1600.9217834472656, 1007.3359322547913, 1760.2136993408203, 1182.9970836639404, 1692.9941940307617, 1191.325535774231, 1692.3397064208984, 1375.3992748260498, 1635.9917449951172, 1389.4678688049316, 0.9334714671105837, 0, 2.899597415918265]\n",
      "Input data length: 25, expected length: 25\n",
      "Predicted Class: 1, Probabilities: [   0.039211     0.59761     0.36318]\n",
      "YOLO results: [[     253.78      266.73      340.65      432.16]]\n",
      "Classes: [          0]\n",
      "Confidences: [    0.91865]\n",
      "Transformed coordinates: (1523.4532470703125, 900.1358184814453, 2047.701416015625, 1458.5607147216797)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 214\u001b[0m\n\u001b[0;32m    211\u001b[0m x1, y1, x2, y2, results, bbox_info \u001b[38;5;241m=\u001b[39m detect_yolo(frame)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m results_pose \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results_pose\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[0;32m    218\u001b[0m     landmarks\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[lm\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m*\u001b[39m width , lm\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m*\u001b[39m height , lm\u001b[38;5;241m.\u001b[39mz] \u001b[38;5;28;01mfor\u001b[39;00m lm \u001b[38;5;129;01min\u001b[39;00m results_pose\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solution_base.py:325\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m RGB_CHANNELS:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel rgb data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 325\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_packet_to_input_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpacket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_stream_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulated_timestamp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    331\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    332\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    333\u001b[0m                                data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 랜드마크 인덱스 정의 (예: 코, 왼쪽 어깨, 오른쪽 어깨 등)\n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]  # 총 11개 랜드마크\n",
    "DR = 'E'\n",
    "MODEL_INPUT = {\n",
    "    'bbox_xyxy': False,\n",
    "    'bbox_ratio': True,\n",
    "    'bbox_class': True,\n",
    "    'head_torso_speed': True,\n",
    "}\n",
    "input_size = 22 + MODEL_INPUT['bbox_xyxy']*4 + MODEL_INPUT['bbox_ratio']*1 + MODEL_INPUT['bbox_class']*1 + MODEL_INPUT['head_torso_speed']*1\n",
    "print('input_size: ', input_size)\n",
    "\n",
    "cls_filename_list = ['N', 'BY', 'FY', 'SY']\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# GRU 모델 정의 및 로드 \n",
    "class GRUModel(torch.nn.Module):\n",
    "    def __init__(self, input_size=input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size = 64\n",
    "        self.num_layers = num_layers = 2\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,\n",
    "                          dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, 3)  # output_size를 직접 지정합니다.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# GRU 모델 로드\n",
    "gru_model = GRUModel(input_size=input_size)\n",
    "gru_model.load_state_dict(torch.load(DR + r':\\project\\CVProject\\results\\my_model_pt\\GRU_Final_ratio_class_speed_normalgru.pt', map_location=torch.device('cpu')))\n",
    "gru_model.eval()\n",
    "\n",
    "# 클래스 이름 정의\n",
    "class_names = {0: 'Normal', 1: 'Fall', 2: 'Danger'}\n",
    "\n",
    "\n",
    "# 기존의 train, valid에서 사용된 데이터를 제외 & 원하는 개수로 N, BY, SY, FY의 비율이 같도록 샘플 추출\n",
    "def test_data_sample(test_root, trvl_data_root, test_num):\n",
    "    except_data_list = list(map(lambda x: x.split('\\\\')[-1].replace('.json', '.mp4'), glob.glob(f'{trvl_data_root}\\\\*\\\\*')))\n",
    "    raw_test_list = list(map(lambda x: x.split('\\\\')[-1], glob.glob(f'{test_root}\\\\*')))\n",
    "    print('raw_test_list_len: ', len(raw_test_list))\n",
    "    test_list = list(set(raw_test_list) - set(except_data_list))\n",
    "    print('test_list len: ', len(test_list))\n",
    "    test_dict = {\n",
    "        x: random.sample([i for i in test_list if i.split('_')[-2] == x], test_num // 4) \n",
    "        for x in cls_filename_list\n",
    "        }\n",
    "    print('test_dict length: ', [f'{i}: {len(test_dict[i])}' for i in cls_filename_list])\n",
    "    return test_dict\n",
    "\n",
    "\n",
    "def calculate_head_upper_body_speed(keypoints, prev_keypoints):\n",
    "    h = np.array([keypoints[0, 0], keypoints[0, 1]])   # 머리 좌표\n",
    "    l = np.array([keypoints[11, 0], keypoints[11, 1]])  # 왼쪽 어깨 좌표\n",
    "    r = np.array([keypoints[12, 0], keypoints[12, 1]])  # 오른쪽 어깨 좌표\n",
    "\n",
    "    # 이전 프레임의 좌표\n",
    "    prev_h = np.array([prev_keypoints[0, 0], prev_keypoints[0, 1]])\n",
    "    prev_l = np.array([prev_keypoints[11, 0], prev_keypoints[11, 1]])\n",
    "    prev_r = np.array([prev_keypoints[12, 0], prev_keypoints[12, 1]])\n",
    "\n",
    "    # 현재 프레임과 이전 프레임의 상체 중심 계산\n",
    "    center_new = (h + l + r) / 3\n",
    "    center_prev = (prev_h + prev_l + prev_r) / 3\n",
    "\n",
    "    # 유클리드 거리 계산 (속도)\n",
    "    speed = distance.euclidean(center_new, center_prev)\n",
    "    return speed\n",
    "\n",
    "\n",
    "def process_landmarks(landmarks): \n",
    "    selected_landmarks = landmarks[LANDMARKS]   # 지정된 랜드마크 선택 \n",
    "    return selected_landmarks[:, :2].flatten()   # (x,y) 좌표 반환\n",
    "\n",
    "\n",
    "def detect_yolo(frame):\n",
    "    # YOLO 모델 로드\n",
    "    yolo_model = YOLO(DR + r':\\project\\CVProject\\best.pt', verbose=False)\n",
    "    # 비디오 프레임을 YOLO 입력 크기로 리사이즈\n",
    "    resized_frame = cv2.resize(frame, (640, 640))\n",
    "    \n",
    "    # YOLO를 사용하여 바운딩 박스 예측\n",
    "    results = yolo_model(resized_frame, verbose=False)\n",
    "    del yolo_model\n",
    "   \n",
    "    # YOLO 예측 결과에서 바운딩 박스 정보 가져오기 \n",
    "    bbox_info=results[0].boxes.xyxy.cpu().numpy() if results and len(results[0].boxes) > 0 else None\n",
    "    \n",
    "    if bbox_info is None or len(bbox_info) == 0:\n",
    "       print(\"No bounding boxes detected.\")\n",
    "       return None, None, None, None, None, None\n",
    "\n",
    "    # 첫 번째 바운딩 박스 정보 가져오기 (여러 개가 있을 경우 첫 번째만 사용)\n",
    "    x1 , y1 , x2 , y2 = bbox_info[0]  \n",
    "    \n",
    "    # 바운딩 박스 좌표를 원본 프레임에 맞게 변환 (640x640에서 원본 크기로)\n",
    "    original_width = frame.shape[1]\n",
    "    original_height = frame.shape[0]\n",
    "    x1, y1, x2, y2 = bbox_info[0]\n",
    "    \n",
    "    x1 *= original_width / 640.0\n",
    "    x2 *= original_width / 640.0\n",
    "    y1 *= original_height / 640.0\n",
    "    y2 *= original_height / 640.0\n",
    "    print(f\"Transformed coordinates: {(x1, y1, x2, y2)}\")\n",
    "    return x1, y1, x2, y2, results, bbox_info\n",
    "\n",
    "\n",
    "def detect_fall(landmarks, prev_landmarks, x1, y1, x2, y2):\n",
    "    speed = calculate_head_upper_body_speed(landmarks, prev_landmarks) if prev_landmarks is not None else 0\n",
    "    processed_landmarks = process_landmarks(landmarks)\n",
    "\n",
    "    bbox_xyxy = [x1, y1, x2, y2]\n",
    "\n",
    "    bbox_width = x2 - x1 \n",
    "    bbox_height = y2 - y1  \n",
    "   \n",
    "    bbox_ratio = bbox_width / bbox_height if bbox_height != 0 else float('inf')\n",
    "   \n",
    "    # 클래스 결정 \n",
    "    if bbox_ratio <= 1.3:\n",
    "       bbox_class = 0   # Normal \n",
    "    elif bbox_ratio <=1.7:\n",
    "       bbox_class = 2   # Danger \n",
    "    else:\n",
    "       bbox_class = 1   # Fall \n",
    "\n",
    "    # 입력 데이터 구성 \n",
    "    input_data = list(processed_landmarks)\n",
    "    \n",
    "    if MODEL_INPUT['bbox_xyxy'] == True:\n",
    "        input_data.extend(bbox_xyxy)\n",
    "    if MODEL_INPUT['bbox_ratio'] == True:\n",
    "        input_data.append(bbox_ratio)\n",
    "    if MODEL_INPUT['bbox_class'] == True:\n",
    "        input_data.append(bbox_class)\n",
    "    if MODEL_INPUT['head_torso_speed'] == True:\n",
    "        input_data.append(speed)\n",
    "    print(input_data)\n",
    "\n",
    "    print(f\"Input data length: {len(input_data)}, expected length: {input_size}\")\n",
    "\n",
    "    if len(input_data) != input_size:\n",
    "       print(f\"Warning: input_data length is {len(input_data)}, expected {input_size}\")\n",
    "       return None , None\n",
    "    \n",
    "    input_tensor=torch.FloatTensor(input_data).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "       output=gru_model(input_tensor)\n",
    "\n",
    "    probabilities=torch.softmax(output , dim=1).cpu().numpy()[0]  \n",
    "    predicted_class=torch.argmax(output).item()\n",
    "    \n",
    "    return predicted_class , probabilities\n",
    "\n",
    " \n",
    "# 비디오 파일 경로 지정\n",
    "trvl_root = DR + r':\\addition_yolobbox_json_6'\n",
    "test_root = DR + r':\\project\\New_Data\\Video\\videos'\n",
    "test_dict = test_data_sample(test_root, trvl_root, 20)\n",
    "test_list = np.array(list(test_dict.values())).flatten().tolist()\n",
    "\n",
    "for i, video_path in enumerate(test_list):\n",
    "    print(i, video_path)\n",
    "    # 비디오 파일 열기\n",
    "    cap = cv2.VideoCapture(test_root + '\\\\' + video_path)\n",
    "\n",
    "    # 비디오 속성 가져오기\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    window_w = int(width * 0.3)\n",
    "    window_h = int(height * 0.3)\n",
    "\n",
    "    cv2.namedWindow('Fall Detection', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Fall Detection', window_w, window_h)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # 출력 비디오 설정\n",
    "    # fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    # out = cv2.VideoWriter('data_video_test_outputY.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "    confidence_threshold = 0.3\n",
    "\n",
    "    previous_bbox = None\n",
    "    previous_label = None\n",
    "    frame_cnt = 0\n",
    "    prev_landmarks=None\n",
    "    # 프레임 처리 루프 \n",
    "    while cap.isOpened():\n",
    "        ret , frame=cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_cnt % 6 != 0:\n",
    "            frame_cnt += 1\n",
    "            continue\n",
    "\n",
    "        frame = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n",
    "        x1, y1, x2, y2, results, bbox_info = detect_yolo(frame)\n",
    "        if x1 == None: break\n",
    "\n",
    "        results_pose = pose.process(frame)\n",
    "\n",
    "        if results_pose.pose_landmarks:\n",
    "            \n",
    "            landmarks=np.array([[lm.x * width , lm.y * height , lm.z] for lm in results_pose.pose_landmarks.landmark])\n",
    "        \n",
    "            if prev_landmarks is not None: \n",
    "                result=detect_fall(landmarks , prev_landmarks, x1, y1, x2, y2)\n",
    "                if result is not None:  \n",
    "                    label , probs = result \n",
    "                    print(f\"Predicted Class: {label}, Probabilities: {probs}\")  \n",
    "                else:\n",
    "                    print(\"Detection failed.\")\n",
    "            else: \n",
    "                label=None \n",
    "\n",
    "            prev_landmarks=landmarks \n",
    "\n",
    "            # 바운딩 박스와 라벨 그리기 \n",
    "            if label is not None and bbox_info is not None and len(bbox_info) > 0:\n",
    "                x1 , y1 , x2 , y2 = bbox_info[0]   \n",
    "                color=(0 ,255 ,0) if label==0 else ((255 ,255 ,0) if label==2 else (255 ,0 ,0)) \n",
    "                cv2.rectangle(frame , (int(x1), int(y1)), (int(x2), int(y2)), color ,2)\n",
    "                class_name=class_names[label] if label is not None else 'Unknown'\n",
    "                cv2.putText(frame , f'GRU: {class_name}' , (int(x1) , int(y1) -10) , cv2.FONT_HERSHEY_SIMPLEX ,0.7 , color ,2)\n",
    "                print(\"YOLO results:\", results[0].boxes.xyxy.cpu().numpy())\n",
    "                print(\"Classes:\", results[0].boxes.cls.cpu().numpy())\n",
    "                print(\"Confidences:\", results[0].boxes.conf.cpu().numpy())\n",
    "            # 랜드마크 표시 \n",
    "            mp_drawing.draw_landmarks(frame , results_pose.pose_landmarks , mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # 프레임 저장 및 출력 \n",
    "        resized_frame=cv2.resize(frame,(1920, 1080))\n",
    "        # out.write(resized_frame) \n",
    "        cv2.imshow('Fall Detection', resized_frame) \n",
    "        if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    # out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU 모델만 사용\n",
    "* bbox의 비율을 기준으로 클래스 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed landmarks length: 22\n",
      "BBox width: 439, height: 358, ratio: 1.2262569832402235, speed: 22.330371340889418\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 441, height: 394, ratio: 1.119289340101523, speed: 7.060977690259665\n",
      "Predicted Class: 1, Probabilities: [   0.037483     0.74019     0.22233]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 484, height: 417, ratio: 1.160671462829736, speed: 56.794296189125006\n",
      "Predicted Class: 1, Probabilities: [   0.023751     0.77615      0.2001]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 389, height: 371, ratio: 1.0485175202156334, speed: 38.520641438712275\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 486, height: 395, ratio: 1.230379746835443, speed: 18.204313168576896\n",
      "Predicted Class: 1, Probabilities: [   0.023676     0.75962     0.21671]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 567, height: 413, ratio: 1.3728813559322033, speed: 1.180409266889902\n",
      "Predicted Class: 1, Probabilities: [   0.020216     0.64278     0.33701]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 463, height: 378, ratio: 1.2248677248677249, speed: 10.389215105249704\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 452, height: 372, ratio: 1.2150537634408602, speed: 8.359958287053491\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 493, height: 409, ratio: 1.2053789731051345, speed: 24.363257791504278\n",
      "Predicted Class: 1, Probabilities: [   0.023677      0.7596     0.21673]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 449, height: 399, ratio: 1.1253132832080202, speed: 14.875019644454314\n",
      "Predicted Class: 1, Probabilities: [   0.039214     0.71736     0.24343]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 511, height: 394, ratio: 1.2969543147208122, speed: 34.77831455170744\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 490, height: 419, ratio: 1.1694510739856803, speed: 58.294960707338376\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77635     0.19991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 535, height: 415, ratio: 1.2891566265060241, speed: 12.640811025272486\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 516, height: 393, ratio: 1.3129770992366412, speed: 64.7742236956106\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77635     0.19991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 508, height: 394, ratio: 1.2893401015228427, speed: 17.720338583889024\n",
      "Predicted Class: 1, Probabilities: [   0.023672     0.75959     0.21673]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 527, height: 360, ratio: 1.4638888888888888, speed: 42.48163618554981\n",
      "Predicted Class: 1, Probabilities: [     0.0268     0.72271     0.25049]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 535, height: 394, ratio: 1.3578680203045685, speed: 36.9267393658044\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 521, height: 395, ratio: 1.3189873417721518, speed: 26.376961503667555\n",
      "Predicted Class: 1, Probabilities: [   0.023057     0.53439     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 522, height: 414, ratio: 1.2608695652173914, speed: 9.033309364449549\n",
      "Predicted Class: 1, Probabilities: [    0.02716     0.57326     0.39958]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 452, height: 417, ratio: 1.0839328537170263, speed: 6.659568440014946\n",
      "Predicted Class: 1, Probabilities: [   0.043174     0.54507     0.41175]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 473, height: 417, ratio: 1.1342925659472423, speed: 4.124104965384208\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 465, height: 445, ratio: 1.0449438202247192, speed: 23.47279252414662\n",
      "Predicted Class: 1, Probabilities: [   0.029126     0.57416     0.39672]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 518, height: 429, ratio: 1.2074592074592074, speed: 10.27668642597605\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 474, height: 420, ratio: 1.1285714285714286, speed: 5.7479763688386685\n",
      "Predicted Class: 1, Probabilities: [   0.027984     0.58306     0.38895]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 493, height: 419, ratio: 1.1766109785202863, speed: 13.293246787166458\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 476, height: 398, ratio: 1.1959798994974875, speed: 10.43288997365509\n",
      "Predicted Class: 1, Probabilities: [   0.027904     0.57898     0.39311]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 452, height: 393, ratio: 1.1501272264631044, speed: 8.593890508484314\n",
      "Predicted Class: 1, Probabilities: [   0.043678     0.59266     0.36366]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 470, height: 393, ratio: 1.1959287531806615, speed: 16.92230142169413\n",
      "Predicted Class: 1, Probabilities: [   0.029819     0.62218       0.348]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 517, height: 395, ratio: 1.308860759493671, speed: 11.587544456619359\n",
      "Predicted Class: 1, Probabilities: [   0.023131     0.53516     0.44171]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 448, height: 395, ratio: 1.1341772151898735, speed: 5.581405822061484\n",
      "Predicted Class: 1, Probabilities: [   0.043671     0.59223      0.3641]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 511, height: 390, ratio: 1.3102564102564103, speed: 9.315064964157136\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 390, ratio: 1.3, speed: 12.15797191266686\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 500, height: 388, ratio: 1.288659793814433, speed: 3.6759463047574803\n",
      "Predicted Class: 1, Probabilities: [   0.029818     0.62224     0.34794]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 380, ratio: 1.3342105263157895, speed: 4.76168224703661\n",
      "Predicted Class: 1, Probabilities: [   0.026816     0.72238     0.25081]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 458, height: 375, ratio: 1.2213333333333334, speed: 2.495326162326622\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 474, height: 375, ratio: 1.264, speed: 14.102319883904734\n",
      "Predicted Class: 1, Probabilities: [   0.042324      0.6842     0.27347]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 462, height: 404, ratio: 1.1435643564356435, speed: 3.9924133847473535\n",
      "Predicted Class: 1, Probabilities: [   0.026788       0.723     0.25022]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 436, height: 367, ratio: 1.1880108991825613, speed: 5.522345393769899\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 457, height: 403, ratio: 1.1339950372208436, speed: 3.877708361272809\n",
      "Predicted Class: 1, Probabilities: [   0.025068     0.75947     0.21546]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 476, height: 433, ratio: 1.0993071593533488, speed: 3.7727309751048397\n",
      "Predicted Class: 1, Probabilities: [   0.027127     0.70987     0.26301]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 430, height: 412, ratio: 1.0436893203883495, speed: 29.47519218634103\n",
      "Predicted Class: 1, Probabilities: [   0.042011     0.69957     0.25841]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 393, height: 377, ratio: 1.0424403183023874, speed: 16.670984340899302\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 414, height: 377, ratio: 1.0981432360742707, speed: 0.6755347085449986\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 508, height: 407, ratio: 1.2481572481572483, speed: 51.696142800794824\n",
      "Predicted Class: 2, Probabilities: [   0.046515     0.46512     0.48837]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 479, height: 344, ratio: 1.3924418604651163, speed: 53.84530779338041\n",
      "Predicted Class: 1, Probabilities: [   0.043265     0.55751     0.39923]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 471, height: 354, ratio: 1.3305084745762712, speed: 42.20452429798461\n",
      "Predicted Class: 1, Probabilities: [   0.043662     0.59263     0.36371]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 469, height: 370, ratio: 1.2675675675675675, speed: 17.867600274797834\n",
      "Predicted Class: 1, Probabilities: [    0.04368     0.59273     0.36359]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 537, height: 395, ratio: 1.3594936708860759, speed: 13.248307833232158\n",
      "Predicted Class: 1, Probabilities: [   0.023078     0.53461     0.44231]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 443, height: 378, ratio: 1.1719576719576719, speed: 71.75326932871685\n",
      "Predicted Class: 1, Probabilities: [   0.018681     0.72363     0.25769]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 463, height: 354, ratio: 1.307909604519774, speed: 17.237341386486058\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 411, height: 405, ratio: 1.0148148148148148, speed: 30.137624053318156\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 354, height: 392, ratio: 0.9030612244897959, speed: 15.343978857408414\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 483, height: 411, ratio: 1.1751824817518248, speed: 35.739215111128516\n",
      "Predicted Class: 1, Probabilities: [   0.023678     0.75958     0.21674]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 489, height: 395, ratio: 1.2379746835443037, speed: 3.5480649666580986\n",
      "Predicted Class: 1, Probabilities: [   0.020051     0.64596     0.33399]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 471, height: 361, ratio: 1.3047091412742382, speed: 83.80972132540428\n",
      "Predicted Class: 1, Probabilities: [   0.026597     0.72061     0.25279]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 516, height: 361, ratio: 1.4293628808864265, speed: 22.324896201769494\n",
      "Predicted Class: 1, Probabilities: [   0.021128     0.70144     0.27743]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 546, height: 391, ratio: 1.3964194373401535, speed: 11.245213070169696\n",
      "Predicted Class: 1, Probabilities: [   0.026816     0.72238      0.2508]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 534, height: 371, ratio: 1.4393530997304582, speed: 32.42277841855936\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 518, height: 408, ratio: 1.2696078431372548, speed: 33.36550013232777\n",
      "Predicted Class: 1, Probabilities: [   0.020214     0.64272     0.33706]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 420, height: 388, ratio: 1.0824742268041236, speed: 8.935506506587092\n",
      "Predicted Class: 1, Probabilities: [   0.032792     0.74774     0.21947]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 448, height: 369, ratio: 1.2140921409214092, speed: 7.796082183379056\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 519, height: 392, ratio: 1.3239795918367347, speed: 1.0968167361991783\n",
      "Predicted Class: 1, Probabilities: [   0.023768      0.7742     0.20203]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 379, ratio: 1.3377308707124012, speed: 123.89206217089097\n",
      "Predicted Class: 2, Probabilities: [   0.017758     0.45331     0.52893]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 521, height: 415, ratio: 1.2554216867469878, speed: 52.89697110573913\n",
      "Predicted Class: 1, Probabilities: [   0.023057     0.53439     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 519, height: 394, ratio: 1.3172588832487309, speed: 50.664411838747455\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 539, height: 409, ratio: 1.3178484107579462, speed: 36.443346965056975\n",
      "Predicted Class: 1, Probabilities: [   0.023057     0.53439     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 523, height: 398, ratio: 1.314070351758794, speed: 32.32799656327519\n",
      "Predicted Class: 1, Probabilities: [   0.023121     0.53505     0.44183]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 516, height: 396, ratio: 1.303030303030303, speed: 26.080633655271438\n",
      "Predicted Class: 1, Probabilities: [   0.023057     0.53439     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 545, height: 401, ratio: 1.3591022443890275, speed: 24.53240056681925\n",
      "Predicted Class: 1, Probabilities: [   0.027852     0.57919     0.39296]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 550, height: 403, ratio: 1.3647642679900744, speed: 12.989422512342756\n",
      "Predicted Class: 1, Probabilities: [   0.023057     0.53439     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 537, height: 407, ratio: 1.3194103194103195, speed: 19.02549022841706\n",
      "Predicted Class: 1, Probabilities: [   0.027526     0.57642     0.39606]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 531, height: 409, ratio: 1.2982885085574571, speed: 30.8420990040688\n",
      "Predicted Class: 1, Probabilities: [   0.023057     0.53439     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 517, height: 406, ratio: 1.2733990147783252, speed: 11.785755542567298\n",
      "Predicted Class: 1, Probabilities: [   0.027187     0.57349     0.39932]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 546, height: 403, ratio: 1.3548387096774193, speed: 4.931647349633118\n",
      "Predicted Class: 1, Probabilities: [    0.02306     0.53443     0.44251]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 532, height: 401, ratio: 1.3266832917705735, speed: 4.7996551957906455\n",
      "Predicted Class: 1, Probabilities: [   0.023057      0.5344     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 546, height: 383, ratio: 1.4255874673629243, speed: 37.5424862416446\n",
      "Predicted Class: 1, Probabilities: [   0.023057     0.53439     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 533, height: 408, ratio: 1.3063725490196079, speed: 10.519901108435747\n",
      "Predicted Class: 1, Probabilities: [   0.027853     0.57924      0.3929]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 521, height: 405, ratio: 1.2864197530864196, speed: 4.043374548290185\n",
      "Predicted Class: 1, Probabilities: [   0.025462     0.68978     0.28475]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 506, height: 409, ratio: 1.2371638141809291, speed: 21.72760923289945\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 502, height: 406, ratio: 1.2364532019704433, speed: 10.500396767977715\n",
      "Predicted Class: 1, Probabilities: [   0.027852      0.5793     0.39284]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 499, height: 411, ratio: 1.2141119221411192, speed: 15.544509879343144\n",
      "Predicted Class: 1, Probabilities: [   0.023062     0.53439     0.44255]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 485, height: 439, ratio: 1.1047835990888382, speed: 24.45381287880794\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 480, height: 414, ratio: 1.1594202898550725, speed: 10.427775907954892\n",
      "Predicted Class: 1, Probabilities: [   0.043161     0.54516     0.41168]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 525, height: 338, ratio: 1.5532544378698225, speed: 133.33512707746166\n",
      "Predicted Class: 2, Probabilities: [   0.035882     0.36314     0.60098]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 513, height: 384, ratio: 1.3359375, speed: 103.70909953721299\n",
      "Predicted Class: 2, Probabilities: [   0.017964     0.45753      0.5245]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 490, height: 452, ratio: 1.084070796460177, speed: 58.610859062902314\n",
      "Predicted Class: 1, Probabilities: [   0.027824     0.57895     0.39323]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 490, height: 421, ratio: 1.163895486935867, speed: 92.02276085127762\n",
      "Predicted Class: 1, Probabilities: [   0.023894     0.77236     0.20375]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 504, height: 412, ratio: 1.2233009708737863, speed: 34.723406368100314\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 499, height: 402, ratio: 1.2412935323383085, speed: 4.567601562638428\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 473, height: 410, ratio: 1.1536585365853658, speed: 9.752377147319557\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 499, height: 404, ratio: 1.2351485148514851, speed: 25.575145011368157\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 504, height: 396, ratio: 1.2727272727272727, speed: 11.962948470659475\n",
      "Predicted Class: 1, Probabilities: [   0.023086     0.53469     0.44223]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 492, height: 390, ratio: 1.2615384615384615, speed: 6.613294753422008\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 493, height: 386, ratio: 1.2772020725388602, speed: 21.49673480804175\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25078]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 486, height: 379, ratio: 1.2823218997361479, speed: 2.6420505194891755\n",
      "Predicted Class: 1, Probabilities: [   0.028197     0.57773     0.39407]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 474, height: 391, ratio: 1.2122762148337596, speed: 3.973282106926879\n",
      "Predicted Class: 1, Probabilities: [   0.042327     0.68417      0.2735]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 474, height: 384, ratio: 1.234375, speed: 7.846062234714124\n",
      "Predicted Class: 1, Probabilities: [   0.043306     0.61192     0.34478]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 489, height: 381, ratio: 1.2834645669291338, speed: 21.799197512657997\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 490, height: 380, ratio: 1.2894736842105263, speed: 3.554295333461321\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 502, height: 384, ratio: 1.3072916666666667, speed: 7.513292911847289\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 505, height: 382, ratio: 1.3219895287958114, speed: 10.213805157184144\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 396, ratio: 1.2803030303030303, speed: 7.576833557879364\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77635     0.19991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 487, height: 411, ratio: 1.1849148418491484, speed: 22.253790332812063\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77635     0.19991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 497, height: 387, ratio: 1.2842377260981912, speed: 62.75180422618157\n",
      "Predicted Class: 1, Probabilities: [     0.0242     0.75637     0.21943]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 449, height: 349, ratio: 1.2865329512893984, speed: 6.006775512306775\n",
      "Predicted Class: 1, Probabilities: [   0.039209     0.71748     0.24331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 483, height: 352, ratio: 1.3721590909090908, speed: 11.027807542169935\n",
      "Predicted Class: 1, Probabilities: [   0.042327     0.68419     0.27349]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 503, height: 359, ratio: 1.4011142061281336, speed: 4.427663923075726\n",
      "Predicted Class: 1, Probabilities: [    0.02682     0.72231     0.25087]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 508, height: 369, ratio: 1.3766937669376693, speed: 12.43302461907966\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 510, height: 375, ratio: 1.36, speed: 9.425152053159785\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 474, height: 350, ratio: 1.3542857142857143, speed: 9.558517656810919\n",
      "Predicted Class: 1, Probabilities: [   0.042301     0.68423     0.27347]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 488, height: 358, ratio: 1.3631284916201116, speed: 2.320565378462131\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 385, height: 349, ratio: 1.1031518624641834, speed: 32.02999995592523\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 403, height: 326, ratio: 1.2361963190184049, speed: 6.886254346854215\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 460, height: 368, ratio: 1.25, speed: 39.15922107308388\n",
      "Predicted Class: 1, Probabilities: [   0.042314     0.68436     0.27333]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 469, height: 384, ratio: 1.2213541666666667, speed: 7.340041955528415\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 434, height: 393, ratio: 1.104325699745547, speed: 28.82128201070896\n",
      "Predicted Class: 1, Probabilities: [   0.042315     0.68422     0.27346]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 466, height: 402, ratio: 1.1592039800995024, speed: 20.002741802457933\n",
      "Predicted Class: 1, Probabilities: [   0.027857      0.5792     0.39295]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 468, height: 404, ratio: 1.1584158415841583, speed: 4.679218685188986\n",
      "Predicted Class: 1, Probabilities: [   0.028175     0.57781     0.39401]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 446, height: 395, ratio: 1.129113924050633, speed: 6.242201486122717\n",
      "Predicted Class: 1, Probabilities: [   0.043174     0.54507     0.41175]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 469, height: 402, ratio: 1.1666666666666667, speed: 2.9651260462474576\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 490, height: 403, ratio: 1.215880893300248, speed: 9.182726514692362\n",
      "Predicted Class: 1, Probabilities: [   0.026817     0.72235     0.25083]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 510, height: 402, ratio: 1.2686567164179106, speed: 5.894441799970179\n",
      "Predicted Class: 1, Probabilities: [   0.026822     0.72211     0.25107]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 501, height: 402, ratio: 1.2462686567164178, speed: 3.816640170995044\n",
      "Predicted Class: 1, Probabilities: [   0.029819     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 496, height: 401, ratio: 1.2369077306733167, speed: 10.334444533055644\n",
      "Predicted Class: 1, Probabilities: [   0.027857      0.5793     0.39284]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 514, height: 400, ratio: 1.285, speed: 6.716738964703216\n",
      "Predicted Class: 1, Probabilities: [   0.027893      0.5804     0.39171]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 512, height: 399, ratio: 1.2832080200501252, speed: 2.3716210389160497\n",
      "Predicted Class: 1, Probabilities: [   0.029054     0.60842     0.36253]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 416, height: 398, ratio: 1.0452261306532664, speed: 2.9745965925228455\n",
      "Predicted Class: 1, Probabilities: [   0.043674       0.593     0.36332]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 491, height: 393, ratio: 1.2493638676844783, speed: 16.60549818303956\n",
      "Predicted Class: 1, Probabilities: [   0.026808     0.72256     0.25063]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 498, height: 389, ratio: 1.2802056555269923, speed: 9.332999022995374\n",
      "Predicted Class: 1, Probabilities: [   0.026736     0.72402     0.24924]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 496, height: 392, ratio: 1.2653061224489797, speed: 20.76242256596296\n",
      "Predicted Class: 1, Probabilities: [   0.023744     0.77625         0.2]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 498, height: 385, ratio: 1.2935064935064935, speed: 25.278018561953754\n",
      "Predicted Class: 1, Probabilities: [     0.0242     0.75637     0.21943]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 502, height: 387, ratio: 1.297157622739018, speed: 4.955676081145265\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77635     0.19991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 502, height: 373, ratio: 1.3458445040214477, speed: 3.2440444504753043\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 506, height: 385, ratio: 1.3142857142857143, speed: 6.532673214469502\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 509, height: 386, ratio: 1.3186528497409327, speed: 25.109615273871405\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 512, height: 384, ratio: 1.3333333333333333, speed: 6.174496988715436\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 512, height: 384, ratio: 1.3333333333333333, speed: 5.556127312905413\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 513, height: 385, ratio: 1.3324675324675326, speed: 31.0290120321115\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 514, height: 374, ratio: 1.374331550802139, speed: 0.5841618813076335\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77635     0.19991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 514, height: 377, ratio: 1.363395225464191, speed: 3.7517830665395535\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77635     0.19991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 516, height: 377, ratio: 1.3687002652519893, speed: 8.02272933546616\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77633     0.19993]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 521, height: 378, ratio: 1.3783068783068784, speed: 20.320170159215586\n",
      "Predicted Class: 1, Probabilities: [   0.024223     0.75562     0.22016]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 505, height: 387, ratio: 1.3049095607235142, speed: 29.762817021766416\n",
      "Predicted Class: 1, Probabilities: [   0.024228     0.75557     0.22021]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 497, height: 373, ratio: 1.3324396782841823, speed: 20.908902235847385\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 502, height: 380, ratio: 1.3210526315789475, speed: 3.404006416029255\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 377, ratio: 1.3448275862068966, speed: 7.774090025627528\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 503, height: 400, ratio: 1.2575, speed: 9.823309419656017\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 504, height: 384, ratio: 1.3125, speed: 6.937810436855063\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 497, height: 379, ratio: 1.3113456464379947, speed: 5.200926426534606\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 496, height: 370, ratio: 1.3405405405405406, speed: 27.585720625060436\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 496, height: 373, ratio: 1.3297587131367292, speed: 17.286228893713204\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 494, height: 388, ratio: 1.2731958762886597, speed: 34.52467502681187\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 509, height: 387, ratio: 1.3152454780361758, speed: 7.46762875082032\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 385, ratio: 1.316883116883117, speed: 31.996762338943785\n",
      "Predicted Class: 1, Probabilities: [   0.029765     0.62411     0.34613]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 383, ratio: 1.3237597911227155, speed: 2.988291532682327\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 499, height: 397, ratio: 1.256926952141058, speed: 11.898990454951702\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 497, height: 395, ratio: 1.2582278481012659, speed: 9.46884164734282\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 498, height: 396, ratio: 1.2575757575757576, speed: 3.0450257004899313\n",
      "Predicted Class: 1, Probabilities: [   0.029819     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 506, height: 392, ratio: 1.2908163265306123, speed: 1.039159632373454\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62218     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 506, height: 387, ratio: 1.3074935400516796, speed: 52.37033984214501\n",
      "Predicted Class: 1, Probabilities: [   0.023982     0.76409     0.21193]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 392, ratio: 1.2933673469387754, speed: 1.8779660441303598\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 391, height: 394, ratio: 0.9923857868020305, speed: 32.286395239009515\n",
      "Predicted Class: 1, Probabilities: [   0.038053     0.73952     0.22243]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 435, height: 380, ratio: 1.144736842105263, speed: 2.9410504624026124\n",
      "Predicted Class: 1, Probabilities: [   0.039214     0.71736     0.24343]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 441, height: 372, ratio: 1.185483870967742, speed: 31.477493268684324\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 478, height: 357, ratio: 1.3389355742296918, speed: 7.31135487611337\n",
      "Predicted Class: 1, Probabilities: [   0.039214     0.71736     0.24343]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 479, height: 391, ratio: 1.225063938618926, speed: 22.809714911322654\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 488, height: 359, ratio: 1.3593314763231197, speed: 6.893396347488828\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 501, height: 365, ratio: 1.3726027397260274, speed: 28.654301829804087\n",
      "Predicted Class: 1, Probabilities: [   0.029823     0.62216     0.34802]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 498, height: 376, ratio: 1.324468085106383, speed: 21.080509062579683\n",
      "Predicted Class: 1, Probabilities: [    0.04368     0.59273     0.36359]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 496, height: 385, ratio: 1.2883116883116883, speed: 17.477691378980293\n",
      "Predicted Class: 1, Probabilities: [    0.04368     0.59273     0.36359]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 495, height: 380, ratio: 1.3026315789473684, speed: 12.460208584775293\n",
      "Predicted Class: 1, Probabilities: [    0.03865     0.60031     0.36104]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 495, height: 366, ratio: 1.3524590163934427, speed: 11.3331509218905\n",
      "Predicted Class: 1, Probabilities: [   0.029816      0.6223     0.34789]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 494, height: 367, ratio: 1.3460490463215258, speed: 4.246667471138797\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 495, height: 373, ratio: 1.3270777479892761, speed: 16.82545483338933\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 508, height: 389, ratio: 1.3059125964010283, speed: 5.9343129804702235\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 507, height: 383, ratio: 1.3237597911227155, speed: 9.948630208881934\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 510, height: 384, ratio: 1.328125, speed: 6.198950706232006\n",
      "Predicted Class: 1, Probabilities: [   0.029805     0.62262     0.34757]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 508, height: 388, ratio: 1.309278350515464, speed: 2.147257067035074\n",
      "Predicted Class: 1, Probabilities: [   0.026816      0.7224     0.25079]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 512, height: 388, ratio: 1.3195876288659794, speed: 2.293960010079141\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 485, height: 390, ratio: 1.2435897435897436, speed: 12.039178744924033\n",
      "Predicted Class: 1, Probabilities: [   0.026815      0.7224     0.25078]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 494, height: 389, ratio: 1.2699228791773778, speed: 0.8470643049838468\n",
      "Predicted Class: 1, Probabilities: [   0.023738     0.77635     0.19991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 492, height: 380, ratio: 1.2947368421052632, speed: 0.6173752173903768\n",
      "Predicted Class: 1, Probabilities: [   0.029819     0.62219     0.34799]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 495, height: 387, ratio: 1.2790697674418605, speed: 8.194400937311578\n",
      "Predicted Class: 1, Probabilities: [   0.024186     0.76179     0.21403]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 502, height: 380, ratio: 1.3210526315789475, speed: 11.012569120381057\n",
      "Predicted Class: 1, Probabilities: [   0.027854     0.57921     0.39294]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 504, height: 383, ratio: 1.3159268929503916, speed: 34.224433807840605\n",
      "Predicted Class: 1, Probabilities: [   0.024875     0.74326     0.23187]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 510, height: 386, ratio: 1.3212435233160622, speed: 6.587258386394559\n",
      "Predicted Class: 1, Probabilities: [   0.026834     0.72161     0.25155]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 512, height: 385, ratio: 1.3298701298701299, speed: 1.50970458984375\n",
      "Predicted Class: 1, Probabilities: [   0.026816     0.72237     0.25082]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 535, height: 385, ratio: 1.3896103896103895, speed: 17.509863658166932\n",
      "Predicted Class: 1, Probabilities: [   0.026618     0.70545     0.26793]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 541, height: 374, ratio: 1.446524064171123, speed: 7.60460336569404\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 519, height: 385, ratio: 1.348051948051948, speed: 6.0819845315231795\n",
      "Predicted Class: 1, Probabilities: [    0.02982     0.62217     0.34801]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 511, height: 381, ratio: 1.3412073490813647, speed: 13.733422717228123\n",
      "Predicted Class: 1, Probabilities: [   0.029797     0.62285     0.34735]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 513, height: 381, ratio: 1.3464566929133859, speed: 34.06171685012266\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 505, height: 383, ratio: 1.318537859007833, speed: 7.664333578471911\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 498, height: 382, ratio: 1.3036649214659686, speed: 30.60483488097088\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 501, height: 385, ratio: 1.3012987012987014, speed: 65.11483793852292\n",
      "Predicted Class: 1, Probabilities: [   0.027703     0.68611     0.28619]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 496, height: 407, ratio: 1.2186732186732188, speed: 21.68015695397868\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 490, height: 404, ratio: 1.2128712871287128, speed: 22.651496353637935\n",
      "Predicted Class: 1, Probabilities: [   0.024224     0.75558      0.2202]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 492, height: 393, ratio: 1.251908396946565, speed: 40.80013591027005\n",
      "Predicted Class: 1, Probabilities: [   0.023676     0.75962     0.21671]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 469, height: 354, ratio: 1.3248587570621468, speed: 15.430357152355844\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 489, height: 388, ratio: 1.2603092783505154, speed: 16.84421311025038\n",
      "Predicted Class: 1, Probabilities: [   0.038052     0.73953     0.22242]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 506, height: 423, ratio: 1.1962174940898345, speed: 20.328878938723594\n",
      "Predicted Class: 1, Probabilities: [   0.026651     0.71785      0.2555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 394, height: 426, ratio: 0.9248826291079812, speed: 15.52222566539949\n",
      "Predicted Class: 1, Probabilities: [   0.028814     0.65997     0.31122]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 471, height: 432, ratio: 1.0902777777777777, speed: 11.900575747656186\n",
      "Predicted Class: 1, Probabilities: [   0.026661     0.71769     0.25565]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 523, height: 446, ratio: 1.1726457399103138, speed: 12.533231970175247\n",
      "Predicted Class: 2, Probabilities: [    0.02445     0.40641     0.56914]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 576, height: 445, ratio: 1.29438202247191, speed: 7.001447978339898\n",
      "Predicted Class: 2, Probabilities: [   0.023846     0.20043     0.77573]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 552, height: 474, ratio: 1.1645569620253164, speed: 22.827863803142375\n",
      "Predicted Class: 2, Probabilities: [   0.024027     0.21928     0.75669]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 491, height: 459, ratio: 1.0697167755991286, speed: 4.483284615044752\n",
      "Predicted Class: 1, Probabilities: [   0.029655     0.63306     0.33728]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 477, height: 482, ratio: 0.9896265560165975, speed: 16.93107691450095\n",
      "Predicted Class: 1, Probabilities: [   0.023676     0.75962     0.21671]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 458, height: 487, ratio: 0.9404517453798767, speed: 14.419974375996608\n",
      "Predicted Class: 1, Probabilities: [   0.026217     0.72438      0.2494]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 441, height: 501, ratio: 0.8802395209580839, speed: 22.43606872562461\n",
      "Predicted Class: 1, Probabilities: [   0.023677      0.7596     0.21672]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 442, height: 516, ratio: 0.8565891472868217, speed: 9.731363955801172\n",
      "Predicted Class: 1, Probabilities: [   0.023676     0.75962     0.21671]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 439, height: 524, ratio: 0.8377862595419847, speed: 6.071841772148475\n",
      "Predicted Class: 1, Probabilities: [   0.023676     0.75962     0.21671]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 423, height: 543, ratio: 0.7790055248618785, speed: 10.56546555583241\n",
      "Predicted Class: 1, Probabilities: [   0.023676     0.75962     0.21671]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 403, height: 550, ratio: 0.7327272727272728, speed: 6.988316010147168\n",
      "Predicted Class: 1, Probabilities: [   0.023676     0.75962     0.21671]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 391, height: 560, ratio: 0.6982142857142857, speed: 4.273904982392933\n",
      "Predicted Class: 1, Probabilities: [   0.023676     0.75962     0.21671]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 378, height: 573, ratio: 0.6596858638743456, speed: 8.923880707945505\n",
      "Predicted Class: 1, Probabilities: [   0.023798     0.75802     0.21818]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 370, height: 584, ratio: 0.6335616438356164, speed: 7.601655277645667\n",
      "Predicted Class: 1, Probabilities: [   0.026542     0.71951     0.25395]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 359, height: 591, ratio: 0.6074450084602369, speed: 5.407392167006907\n",
      "Predicted Class: 1, Probabilities: [   0.026659     0.71772     0.25562]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 347, height: 599, ratio: 0.5792988313856428, speed: 11.312237492674182\n",
      "Predicted Class: 1, Probabilities: [   0.026659     0.71772     0.25562]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 335, height: 611, ratio: 0.5482815057283142, speed: 10.751819385035041\n",
      "Predicted Class: 1, Probabilities: [   0.026659     0.71772     0.25562]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 326, height: 619, ratio: 0.5266558966074314, speed: 8.365667686508322\n",
      "Predicted Class: 1, Probabilities: [   0.026659     0.71772     0.25562]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 314, height: 628, ratio: 0.5, speed: 6.5388268984706\n",
      "Predicted Class: 1, Probabilities: [   0.026659     0.71772     0.25562]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 305, height: 640, ratio: 0.4765625, speed: 8.706761279912985\n",
      "Predicted Class: 1, Probabilities: [   0.026659     0.71772     0.25562]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 296, height: 647, ratio: 0.4574961360123648, speed: 8.610952095427278\n",
      "Predicted Class: 1, Probabilities: [   0.026659     0.71772     0.25562]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 279, height: 654, ratio: 0.42660550458715596, speed: 8.895344304352475\n",
      "Predicted Class: 1, Probabilities: [   0.026659     0.71772     0.25562]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 265, height: 665, ratio: 0.39849624060150374, speed: 10.618621663271366\n",
      "Predicted Class: 1, Probabilities: [   0.026663      0.7175     0.25583]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 255, height: 666, ratio: 0.38288288288288286, speed: 9.656294993367453\n",
      "Predicted Class: 1, Probabilities: [   0.027121     0.69339     0.27949]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 249, height: 675, ratio: 0.3688888888888889, speed: 6.1408604519847785\n",
      "Predicted Class: 1, Probabilities: [   0.027967      0.6538     0.31823]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 243, height: 680, ratio: 0.3573529411764706, speed: 8.645154982409867\n",
      "Predicted Class: 1, Probabilities: [   0.028998     0.60569     0.36532]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 233, height: 681, ratio: 0.342143906020558, speed: 6.7195088519619715\n",
      "Predicted Class: 1, Probabilities: [   0.027912     0.65631     0.31577]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 233, height: 684, ratio: 0.3406432748538012, speed: 9.358104150328144\n",
      "Predicted Class: 1, Probabilities: [   0.027587     0.67127     0.30114]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 246, height: 683, ratio: 0.3601756954612006, speed: 5.959127832701963\n",
      "Predicted Class: 1, Probabilities: [   0.027587     0.67127     0.30114]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 258, height: 689, ratio: 0.37445573294629897, speed: 6.211183328595888\n",
      "Predicted Class: 1, Probabilities: [   0.027587     0.67127     0.30114]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 270, height: 694, ratio: 0.38904899135446686, speed: 7.998437152579105\n",
      "Predicted Class: 1, Probabilities: [   0.027587     0.67127     0.30114]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 275, height: 694, ratio: 0.3962536023054755, speed: 6.854698919959618\n",
      "Predicted Class: 1, Probabilities: [   0.027587     0.67127     0.30114]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 285, height: 696, ratio: 0.40948275862068967, speed: 5.607581572631796\n",
      "Predicted Class: 1, Probabilities: [   0.027587     0.67127     0.30114]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 306, height: 695, ratio: 0.44028776978417267, speed: 4.292415663553605\n",
      "Predicted Class: 1, Probabilities: [   0.027587     0.67127     0.30114]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 316, height: 691, ratio: 0.4573082489146165, speed: 6.9311710068191035\n",
      "Predicted Class: 1, Probabilities: [   0.027587     0.67127     0.30114]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 332, height: 689, ratio: 0.4818577648766328, speed: 7.581856760173807\n",
      "Predicted Class: 1, Probabilities: [    0.02106     0.77445     0.20449]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 336, height: 682, ratio: 0.49266862170087977, speed: 9.205860436418007\n",
      "Predicted Class: 1, Probabilities: [   0.024663     0.72474      0.2506]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 347, height: 679, ratio: 0.5110456553755522, speed: 8.310106248004601\n",
      "Predicted Class: 1, Probabilities: [   0.021399     0.77024     0.20836]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 352, height: 680, ratio: 0.5176470588235295, speed: 5.745519909734456\n",
      "Predicted Class: 1, Probabilities: [   0.020924     0.77613     0.20295]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 350, height: 680, ratio: 0.5147058823529411, speed: 12.720374945687754\n",
      "Predicted Class: 1, Probabilities: [   0.020924     0.77613     0.20295]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 365, height: 681, ratio: 0.5359765051395007, speed: 4.214549616541668\n",
      "Predicted Class: 1, Probabilities: [   0.020924     0.77613     0.20295]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 359, height: 681, ratio: 0.527165932452276, speed: 9.706005915840942\n",
      "Predicted Class: 1, Probabilities: [   0.020924     0.77613     0.20295]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 356, height: 680, ratio: 0.5235294117647059, speed: 8.299976796958877\n",
      "Predicted Class: 1, Probabilities: [   0.020925     0.77613     0.20295]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 353, height: 676, ratio: 0.522189349112426, speed: 6.522859426027282\n",
      "Predicted Class: 1, Probabilities: [   0.020937     0.77612     0.20295]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 357, height: 664, ratio: 0.5376506024096386, speed: 8.578337510175052\n",
      "Predicted Class: 1, Probabilities: [   0.032171     0.76365     0.20418]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 352, height: 656, ratio: 0.5365853658536586, speed: 10.250637174876141\n",
      "Predicted Class: 1, Probabilities: [   0.032222     0.76362     0.20416]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 350, height: 663, ratio: 0.5279034690799397, speed: 9.71498214800749\n",
      "Predicted Class: 1, Probabilities: [   0.033324     0.75033     0.21635]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 329, height: 663, ratio: 0.4962292609351433, speed: 8.919435214649011\n",
      "Predicted Class: 1, Probabilities: [   0.032794     0.72664     0.24057]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 656, ratio: 0.4771341463414634, speed: 4.133934986933051\n",
      "Predicted Class: 1, Probabilities: [   0.032384     0.70847     0.25914]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 293, height: 655, ratio: 0.44732824427480916, speed: 1.9872487959196878\n",
      "Predicted Class: 1, Probabilities: [   0.043236     0.54619     0.41057]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 284, height: 654, ratio: 0.43425076452599387, speed: 6.516003707188532\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 279, height: 654, ratio: 0.42660550458715596, speed: 6.444673503034746\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 263, height: 656, ratio: 0.4009146341463415, speed: 7.162678132310167\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 231, height: 656, ratio: 0.3521341463414634, speed: 9.748782917648745\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 215, height: 656, ratio: 0.3277439024390244, speed: 10.585620311159987\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 209, height: 653, ratio: 0.32006125574272587, speed: 9.555784221354594\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 206, height: 662, ratio: 0.311178247734139, speed: 11.649149979645655\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 197, height: 659, ratio: 0.29893778452200304, speed: 11.962451749109523\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 193, height: 656, ratio: 0.2942073170731707, speed: 8.943833807669566\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 184, height: 652, ratio: 0.2822085889570552, speed: 5.937183725141554\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 183, height: 650, ratio: 0.2815384615384615, speed: 6.324551637843066\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 185, height: 644, ratio: 0.28726708074534163, speed: 11.046849117422022\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 181, height: 642, ratio: 0.2819314641744548, speed: 9.86619311895785\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 181, height: 634, ratio: 0.2854889589905363, speed: 6.256388597978364\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 186, height: 632, ratio: 0.29430379746835444, speed: 9.155319541025309\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 191, height: 627, ratio: 0.30462519936204147, speed: 7.4149445332204085\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 195, height: 625, ratio: 0.312, speed: 9.693906640023005\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 196, height: 624, ratio: 0.3141025641025641, speed: 4.488799244783524\n",
      "Predicted Class: 1, Probabilities: [   0.043215     0.54646     0.41032]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 194, height: 615, ratio: 0.3154471544715447, speed: 8.000589375469294\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61576     0.34556]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 195, height: 615, ratio: 0.3170731707317073, speed: 9.776612430392998\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 176, height: 614, ratio: 0.28664495114006516, speed: 6.600545293959618\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 182, height: 613, ratio: 0.2969004893964111, speed: 7.964047384151662\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 187, height: 610, ratio: 0.30655737704918035, speed: 4.222273654275912\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 189, height: 611, ratio: 0.309328968903437, speed: 5.876189132029398\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 197, height: 618, ratio: 0.3187702265372168, speed: 8.714270913079693\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 204, height: 615, ratio: 0.33170731707317075, speed: 7.823204703903462\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 210, height: 615, ratio: 0.34146341463414637, speed: 6.480113852398644\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 205, height: 615, ratio: 0.3333333333333333, speed: 5.773710228356128\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 203, height: 621, ratio: 0.32689210950080516, speed: 7.054089921233186\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 202, height: 623, ratio: 0.32423756019261635, speed: 4.964006049542716\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 208, height: 631, ratio: 0.329635499207607, speed: 5.647268258206236\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 212, height: 638, ratio: 0.3322884012539185, speed: 7.60826873301448\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 217, height: 643, ratio: 0.33748055987558323, speed: 6.890742692106156\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 225, height: 645, ratio: 0.3488372093023256, speed: 4.403364138955972\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 232, height: 639, ratio: 0.36306729264475746, speed: 6.565465271673243\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 243, height: 636, ratio: 0.38207547169811323, speed: 1.6489276544977043\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 245, height: 633, ratio: 0.38704581358609796, speed: 3.9003352846649824\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 253, height: 636, ratio: 0.3977987421383648, speed: 5.897213087640223\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 255, height: 644, ratio: 0.39596273291925466, speed: 2.2398605502458624\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 254, height: 635, ratio: 0.4, speed: 7.310337834772182\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 253, height: 589, ratio: 0.4295415959252971, speed: 5.637671706455878\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 256, height: 585, ratio: 0.4376068376068376, speed: 10.126748373423473\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 252, height: 591, ratio: 0.4263959390862944, speed: 9.126621517923072\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 247, height: 607, ratio: 0.40691927512355847, speed: 10.497994611754352\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 252, height: 607, ratio: 0.41515650741350907, speed: 9.181283582806467\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 250, height: 606, ratio: 0.41254125412541254, speed: 6.602472333534251\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 260, height: 590, ratio: 0.4406779661016949, speed: 8.142482037519317\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 264, height: 574, ratio: 0.45993031358885017, speed: 7.217009569686144\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 257, height: 550, ratio: 0.4672727272727273, speed: 13.89699714639896\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 268, height: 533, ratio: 0.5028142589118199, speed: 8.090130574331045\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 259, height: 512, ratio: 0.505859375, speed: 8.362985869076486\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 269, height: 487, ratio: 0.5523613963039015, speed: 6.820301504395235\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 254, height: 472, ratio: 0.538135593220339, speed: 3.691175317060126\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 268, height: 461, ratio: 0.5813449023861171, speed: 6.11457051022709\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 264, height: 455, ratio: 0.5802197802197803, speed: 7.531096510851839\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 258, height: 443, ratio: 0.582392776523702, speed: 6.697924552977989\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34554]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 264, height: 433, ratio: 0.6096997690531177, speed: 7.196596812841177\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34554]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 282, height: 435, ratio: 0.6482758620689655, speed: 1.2796543268485592\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 290, height: 434, ratio: 0.6682027649769585, speed: 1.9795200894892873\n",
      "Predicted Class: 1, Probabilities: [   0.041824     0.63778     0.32039]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 293, height: 426, ratio: 0.687793427230047, speed: 4.66566022214639\n",
      "Predicted Class: 1, Probabilities: [   0.038967     0.61798     0.34305]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 283, height: 426, ratio: 0.6643192488262911, speed: 2.807914343390061\n",
      "Predicted Class: 1, Probabilities: [   0.039798      0.6241     0.33611]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 273, height: 433, ratio: 0.6304849884526559, speed: 3.8509335401670675\n",
      "Predicted Class: 1, Probabilities: [   0.038959     0.61792     0.34313]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 293, height: 434, ratio: 0.6751152073732719, speed: 3.472143836682748\n",
      "Predicted Class: 1, Probabilities: [   0.042845     0.64406     0.31309]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 300, height: 434, ratio: 0.6912442396313364, speed: 3.9510999040435735\n",
      "Predicted Class: 1, Probabilities: [   0.042542     0.64224     0.31522]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 270, height: 437, ratio: 0.6178489702517163, speed: 3.977207888048823\n",
      "Predicted Class: 1, Probabilities: [   0.043038      0.6452     0.31176]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 276, height: 439, ratio: 0.6287015945330297, speed: 0.9590558341234365\n",
      "Predicted Class: 1, Probabilities: [   0.043254     0.64647     0.31028]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 263, height: 443, ratio: 0.5936794582392777, speed: 3.6264057132815433\n",
      "Predicted Class: 1, Probabilities: [   0.042602      0.6426      0.3148]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 304, height: 451, ratio: 0.6740576496674058, speed: 4.373999185411266\n",
      "Predicted Class: 1, Probabilities: [   0.038884     0.61735     0.34377]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 268, height: 459, ratio: 0.5838779956427015, speed: 4.654186546451596\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 303, height: 467, ratio: 0.6488222698072805, speed: 0.9609049468380212\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 321, height: 481, ratio: 0.6673596673596673, speed: 2.815130180695301\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 325, height: 490, ratio: 0.6632653061224489, speed: 2.539810693555347\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 325, height: 493, ratio: 0.6592292089249493, speed: 2.275787831305474\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 327, height: 498, ratio: 0.6566265060240963, speed: 0.8870326697566897\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 326, height: 498, ratio: 0.6546184738955824, speed: 1.961073112843823\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 326, height: 503, ratio: 0.6481113320079522, speed: 0.8249933326437645\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 324, height: 512, ratio: 0.6328125, speed: 2.4022615705502233\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 322, height: 520, ratio: 0.6192307692307693, speed: 1.154119645553563\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 322, height: 524, ratio: 0.6145038167938931, speed: 2.255274569655031\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 321, height: 521, ratio: 0.6161228406909789, speed: 2.5068450983654325\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 316, height: 524, ratio: 0.6030534351145038, speed: 3.6011406884281967\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 312, height: 526, ratio: 0.5931558935361216, speed: 3.1265824558707718\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 309, height: 528, ratio: 0.5852272727272727, speed: 2.8491978886168847\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 308, height: 532, ratio: 0.5789473684210527, speed: 1.6923737631590938\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 533, ratio: 0.5609756097560976, speed: 2.4441728832112384\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 296, height: 536, ratio: 0.5522388059701493, speed: 3.1112989452729014\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 291, height: 535, ratio: 0.5439252336448598, speed: 2.6171085457043235\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 283, height: 540, ratio: 0.524074074074074, speed: 2.055605684563651\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 278, height: 538, ratio: 0.516728624535316, speed: 1.3505726489365855\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 270, height: 531, ratio: 0.5084745762711864, speed: 4.6844946255309905\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34554]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 267, height: 518, ratio: 0.5154440154440154, speed: 5.976522718085707\n",
      "Predicted Class: 1, Probabilities: [   0.043267     0.64654     0.31019]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 259, height: 502, ratio: 0.5159362549800797, speed: 4.77590163603848\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 243, height: 490, ratio: 0.4959183673469388, speed: 3.461065713895039\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 245, height: 485, ratio: 0.5051546391752577, speed: 5.057736622681106\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 246, height: 473, ratio: 0.5200845665961945, speed: 3.9267942504377302\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 237, height: 465, ratio: 0.5096774193548387, speed: 4.764323039011036\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 237, height: 455, ratio: 0.5208791208791209, speed: 3.329811884417826\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 235, height: 452, ratio: 0.5199115044247787, speed: 5.790339099118723\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 213, height: 442, ratio: 0.4819004524886878, speed: 7.151051557420324\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 184, height: 444, ratio: 0.4144144144144144, speed: 1.2137334938304678\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 171, height: 441, ratio: 0.3877551020408163, speed: 1.5542583687750913\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 151, height: 426, ratio: 0.3544600938967136, speed: 1.3887796414237037\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 126, height: 423, ratio: 0.2978723404255319, speed: 6.432745006366154\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 131, height: 424, ratio: 0.3089622641509434, speed: 3.78189760396649\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 120, height: 421, ratio: 0.2850356294536817, speed: 4.655486332363052\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 141, height: 418, ratio: 0.3373205741626794, speed: 3.129867019315139\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 163, height: 414, ratio: 0.39371980676328505, speed: 3.7864137467742927\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 179, height: 418, ratio: 0.42822966507177035, speed: 6.58465539245372\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 193, height: 421, ratio: 0.4584323040380047, speed: 9.866666504894576\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 175, height: 425, ratio: 0.4117647058823529, speed: 7.057227781608338\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 171, height: 428, ratio: 0.39953271028037385, speed: 8.25714218972582\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 185, height: 447, ratio: 0.41387024608501116, speed: 2.4356099295803917\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 182, height: 448, ratio: 0.40625, speed: 0.676353147147576\n",
      "Predicted Class: 1, Probabilities: [   0.043297     0.64672     0.30999]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 193, height: 454, ratio: 0.4251101321585903, speed: 4.162228409123212\n",
      "Predicted Class: 1, Probabilities: [   0.043293      0.6467     0.31001]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 189, height: 467, ratio: 0.40471092077087795, speed: 9.556977588614501\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 190, height: 473, ratio: 0.40169133192389006, speed: 5.997856976544778\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 190, height: 480, ratio: 0.3958333333333333, speed: 6.709844929461483\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 191, height: 483, ratio: 0.39544513457556935, speed: 3.744467385999182\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 212, height: 483, ratio: 0.4389233954451346, speed: 0.651733931770606\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 224, height: 485, ratio: 0.4618556701030928, speed: 4.28282218517331\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 232, height: 487, ratio: 0.47638603696098564, speed: 2.950286539270011\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 237, height: 491, ratio: 0.48268839103869654, speed: 9.532426291066434\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 234, height: 506, ratio: 0.4624505928853755, speed: 16.84789778016534\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 243, height: 507, ratio: 0.47928994082840237, speed: 1.9014168135060756\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 263, height: 521, ratio: 0.5047984644913628, speed: 9.086468229803947\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 271, height: 537, ratio: 0.5046554934823091, speed: 10.54601948250746\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 277, height: 537, ratio: 0.515828677839851, speed: 2.7983119202882216\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 285, height: 520, ratio: 0.5480769230769231, speed: 6.991997137123482\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 287, height: 528, ratio: 0.5435606060606061, speed: 7.68470948604037\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 292, height: 531, ratio: 0.5499058380414312, speed: 4.042997686899386\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 294, height: 569, ratio: 0.5166959578207382, speed: 15.732383889779454\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 539, ratio: 0.5547309833024119, speed: 6.613009548622363\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 298, height: 558, ratio: 0.5340501792114696, speed: 6.407295969242645\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 297, height: 554, ratio: 0.5361010830324909, speed: 1.7023961286816973\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 559, ratio: 0.5348837209302325, speed: 2.84200919979793\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 565, ratio: 0.5292035398230088, speed: 2.8065179845428694\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 566, ratio: 0.5335689045936396, speed: 5.151873698232251\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 570, ratio: 0.5298245614035088, speed: 5.82260464243538\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 576, ratio: 0.5243055555555556, speed: 3.1566676697732095\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 581, ratio: 0.5197934595524957, speed: 2.1144727215881285\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 588, ratio: 0.5136054421768708, speed: 0.5237481239847646\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 301, height: 594, ratio: 0.5067340067340067, speed: 4.308583798748987\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 593, ratio: 0.5092748735244519, speed: 2.8612289341218013\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 303, height: 593, ratio: 0.5109612141652614, speed: 0.4112180984673447\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 303, height: 596, ratio: 0.5083892617449665, speed: 3.082708562098607\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 598, ratio: 0.5050167224080268, speed: 5.20285412700529\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 600, ratio: 0.5033333333333333, speed: 0.80614580839566\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 605, ratio: 0.4991735537190083, speed: 3.3316901840129542\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61578     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 304, height: 609, ratio: 0.49917898193760263, speed: 1.7943080541095475\n",
      "Predicted Class: 1, Probabilities: [   0.038678     0.61577     0.34555]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 305, height: 615, ratio: 0.4959349593495935, speed: 2.488081219478741\n",
      "Predicted Class: 1, Probabilities: [   0.038685     0.61561     0.34571]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 309, height: 615, ratio: 0.5024390243902439, speed: 1.6804643726102482\n",
      "Predicted Class: 1, Probabilities: [   0.039216     0.60487     0.35591]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 310, height: 613, ratio: 0.5057096247960848, speed: 1.4780587351866865\n",
      "Predicted Class: 1, Probabilities: [   0.038813     0.61267     0.34852]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 310, height: 613, ratio: 0.5057096247960848, speed: 0.9915169143501991\n",
      "Predicted Class: 1, Probabilities: [     0.0392     0.60516     0.35564]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 310, height: 614, ratio: 0.504885993485342, speed: 0.750798005017588\n",
      "Predicted Class: 1, Probabilities: [   0.041965     0.56352     0.39452]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 308, height: 617, ratio: 0.4991896272285251, speed: 0.6837964142714104\n",
      "Predicted Class: 1, Probabilities: [   0.043182     0.54691     0.40991]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 308, height: 622, ratio: 0.49517684887459806, speed: 0.30734764156481753\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54619     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 309, height: 623, ratio: 0.4959871589085072, speed: 3.487404573777869\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 309, height: 624, ratio: 0.4951923076923077, speed: 3.8170244221601264\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 310, height: 628, ratio: 0.49363057324840764, speed: 3.8659320421312406\n",
      "Predicted Class: 1, Probabilities: [   0.043236     0.54618     0.41058]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 311, height: 628, ratio: 0.49522292993630573, speed: 3.80153881531514\n",
      "Predicted Class: 1, Probabilities: [   0.043245     0.54636     0.41039]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 311, height: 635, ratio: 0.48976377952755906, speed: 1.9108565945603542\n",
      "Predicted Class: 1, Probabilities: [   0.050837     0.61809     0.33107]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 311, height: 641, ratio: 0.48517940717628705, speed: 1.458174541562465\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 311, height: 645, ratio: 0.4821705426356589, speed: 1.4882969642530823\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 647, ratio: 0.48377125193199383, speed: 0.844496329478752\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 653, ratio: 0.4793261868300153, speed: 0.7333310216672929\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 314, height: 653, ratio: 0.48085758039816234, speed: 3.929389982930985\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 314, height: 653, ratio: 0.48085758039816234, speed: 4.722963079700281\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 316, height: 652, ratio: 0.48466257668711654, speed: 1.472526294471755\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 316, height: 655, ratio: 0.48244274809160304, speed: 1.386422706047397\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 661, ratio: 0.4735249621785174, speed: 0.252977072581692\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 312, height: 661, ratio: 0.4720121028744327, speed: 1.638850019566335\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 312, height: 660, ratio: 0.4727272727272727, speed: 1.067567399854274\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 662, ratio: 0.472809667673716, speed: 1.5650910699121272\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 664, ratio: 0.4713855421686747, speed: 1.7316866309608367\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 314, height: 665, ratio: 0.47218045112781953, speed: 1.6060893296957666\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 666, ratio: 0.46996996996997, speed: 0.6665550938710815\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 669, ratio: 0.4678624813153961, speed: 0.9180061266940658\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 312, height: 671, ratio: 0.46497764530551416, speed: 0.4585394634925136\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 312, height: 670, ratio: 0.46567164179104475, speed: 0.7234458412062801\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 669, ratio: 0.4678624813153961, speed: 0.5102536604576486\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 314, height: 669, ratio: 0.4693572496263079, speed: 1.5773531352902643\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 671, ratio: 0.46646795827123694, speed: 0.9398434123667119\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 314, height: 672, ratio: 0.46726190476190477, speed: 0.7065488795730119\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 314, height: 672, ratio: 0.46726190476190477, speed: 1.057817901549565\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 672, ratio: 0.46577380952380953, speed: 0.7825470629829239\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 672, ratio: 0.46577380952380953, speed: 1.02030795766413\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 673, ratio: 0.4650817236255572, speed: 1.0007118058588746\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 313, height: 673, ratio: 0.4650817236255572, speed: 0.984845909317459\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 310, height: 673, ratio: 0.4606240713224368, speed: 1.0067605617145914\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 309, height: 676, ratio: 0.45710059171597633, speed: 1.055286911263816\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 309, height: 676, ratio: 0.45710059171597633, speed: 0.6980740566946624\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 311, height: 676, ratio: 0.46005917159763315, speed: 1.215280640813632\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 308, height: 678, ratio: 0.45427728613569324, speed: 1.2868167207418268\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 309, height: 678, ratio: 0.4557522123893805, speed: 1.0530754195083523\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 310, height: 678, ratio: 0.45722713864306785, speed: 1.0147444511091825\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 308, height: 678, ratio: 0.45427728613569324, speed: 0.7969762061776463\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 307, height: 680, ratio: 0.4514705882352941, speed: 1.0162510762631616\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 308, height: 681, ratio: 0.4522760646108664, speed: 0.5057744821174318\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 308, height: 680, ratio: 0.45294117647058824, speed: 0.7542483736065709\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 308, height: 680, ratio: 0.45294117647058824, speed: 1.2304696380915168\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 309, height: 679, ratio: 0.45508100147275404, speed: 1.1138830868252885\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 306, height: 681, ratio: 0.44933920704845814, speed: 1.4012172206080762\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 304, height: 681, ratio: 0.44640234948604995, speed: 1.6956467780663371\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 303, height: 681, ratio: 0.44493392070484583, speed: 0.790207691653411\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 303, height: 681, ratio: 0.44493392070484583, speed: 0.2034605095566983\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 305, height: 680, ratio: 0.4485294117647059, speed: 0.5156536149936571\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 306, height: 680, ratio: 0.45, speed: 0.5602659915816348\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 307, height: 680, ratio: 0.4514705882352941, speed: 0.34002386562126513\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 307, height: 680, ratio: 0.4514705882352941, speed: 0.6371243433522438\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 306, height: 680, ratio: 0.45, speed: 0.8113359098286177\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 305, height: 680, ratio: 0.4485294117647059, speed: 0.241657070376682\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 304, height: 680, ratio: 0.4470588235294118, speed: 1.212059672066743\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 303, height: 680, ratio: 0.4455882352941177, speed: 1.025096146690583\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 680, ratio: 0.4441176470588235, speed: 0.36054423758214194\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 680, ratio: 0.4441176470588235, speed: 0.1732970607420287\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 680, ratio: 0.4441176470588235, speed: 0.4731538535359068\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 301, height: 680, ratio: 0.4426470588235294, speed: 0.2066901461348489\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 300, height: 679, ratio: 0.4418262150220913, speed: 0.43204539564731037\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 678, ratio: 0.4410029498525074, speed: 0.9301241834513433\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 298, height: 676, ratio: 0.4408284023668639, speed: 0.32504752383770047\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 298, height: 673, ratio: 0.4427934621099554, speed: 0.5023315038251006\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 671, ratio: 0.4456035767511177, speed: 0.7721441731721105\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 670, ratio: 0.4462686567164179, speed: 0.5947039962092346\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 670, ratio: 0.4462686567164179, speed: 1.3338491245504203\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 668, ratio: 0.4476047904191617, speed: 1.5599959308805849\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 298, height: 668, ratio: 0.44610778443113774, speed: 1.934122613242405\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 298, height: 667, ratio: 0.44677661169415295, speed: 0.47279254340720617\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 297, height: 666, ratio: 0.44594594594594594, speed: 1.7894289675377848\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 296, height: 665, ratio: 0.44511278195488724, speed: 1.0045391554798113\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 297, height: 664, ratio: 0.44728915662650603, speed: 1.1765067104274605\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 662, ratio: 0.4516616314199396, speed: 0.9687979457950829\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 660, ratio: 0.453030303030303, speed: 0.5567039791519826\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 659, ratio: 0.4537177541729894, speed: 0.4162694952102907\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 299, height: 658, ratio: 0.45440729483282677, speed: 1.1180982259205734\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 301, height: 657, ratio: 0.4581430745814307, speed: 0.12505615092712297\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 301, height: 657, ratio: 0.4581430745814307, speed: 0.48954142644202836\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 301, height: 656, ratio: 0.45884146341463417, speed: 0.3299993548578376\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 655, ratio: 0.46106870229007635, speed: 0.6167152771554191\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 302, height: 655, ratio: 0.46106870229007635, speed: 1.148393287254897\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 306, height: 655, ratio: 0.467175572519084, speed: 0.577352117376918\n",
      "Predicted Class: 1, Probabilities: [   0.050843     0.61812     0.33104]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 306, height: 655, ratio: 0.467175572519084, speed: 1.0551173704654844\n",
      "Predicted Class: 1, Probabilities: [   0.050849     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 310, height: 655, ratio: 0.4732824427480916, speed: 1.2539372012969834\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 311, height: 655, ratio: 0.47480916030534354, speed: 1.689437071414031\n",
      "Predicted Class: 1, Probabilities: [    0.05085     0.61815       0.331]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 315, height: 653, ratio: 0.48238897396630936, speed: 0.5567180261319364\n",
      "Predicted Class: 1, Probabilities: [   0.050839      0.6181     0.33106]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 317, height: 653, ratio: 0.48545176110260335, speed: 0.9276074499446463\n",
      "Predicted Class: 1, Probabilities: [   0.050793     0.61789     0.33131]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 320, height: 653, ratio: 0.4900459418070444, speed: 0.7738637728990233\n",
      "Predicted Class: 1, Probabilities: [   0.050388     0.61601      0.3336]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 320, height: 654, ratio: 0.4892966360856269, speed: 0.6535114288914873\n",
      "Predicted Class: 1, Probabilities: [   0.050611     0.61706     0.33233]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 323, height: 653, ratio: 0.4946401225114854, speed: 1.913950012029187\n",
      "Predicted Class: 1, Probabilities: [   0.050349     0.61582     0.33383]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 325, height: 650, ratio: 0.5, speed: 1.0026765944195721\n",
      "Predicted Class: 1, Probabilities: [   0.045607     0.58091     0.37348]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 328, height: 649, ratio: 0.5053929121725732, speed: 0.7747875837938095\n",
      "Predicted Class: 1, Probabilities: [   0.043298     0.54737     0.40933]\n",
      "Processed landmarks length: 22\n",
      "BBox width: 331, height: 645, ratio: 0.5131782945736434, speed: 0.4706430677039\n",
      "Predicted Class: 1, Probabilities: [   0.043235     0.54618     0.41058]\n"
     ]
    }
   ],
   "source": [
    "# 바운딩 박스 계산 및 그리기 함수\n",
    "def calculate_and_draw_bbox(frame, landmarks):\n",
    "    x_coordinates = landmarks[:, 0]\n",
    "    y_coordinates = landmarks[:, 1]\n",
    "    \n",
    "    x1 = max(0, int(np.min(x_coordinates)))\n",
    "    y1 = max(0, int(np.min(y_coordinates)))\n",
    "    x2 = min(frame.shape[1], int(np.max(x_coordinates)))\n",
    "    y2 = min(frame.shape[0], int(np.max(y_coordinates)))\n",
    "    \n",
    "    bbox_width = x2 - x1\n",
    "    bbox_height = y2 - y1\n",
    "    \n",
    "    # 높이가 0일 경우 비율을 무한대로 설정\n",
    "    bbox_ratio = bbox_width / bbox_height if bbox_height != 0 else float('inf')\n",
    "    \n",
    "    # 바운딩 박스 클래스 결정\n",
    "    bbox_class = 0\n",
    "    if bbox_ratio < 0.5:\n",
    "        bbox_class = 0  # Normal\n",
    "    elif 0.5 <= bbox_ratio < 0.7:\n",
    "        bbox_class = 2  # Danger\n",
    "    else:\n",
    "        bbox_class = 1  # Fall\n",
    "    \n",
    "    # 바운딩 박스를 조금 더 넓게 조정 (각 방향으로 50픽셀 추가)\n",
    "    padding = 50\n",
    "    x1 = max(0, x1 - padding)\n",
    "    y1 = max(0, y1 - padding)\n",
    "    x2 = min(frame.shape[1], x2 + padding)\n",
    "    y2 = min(frame.shape[0], y2 + padding)\n",
    "    \n",
    "    return (x1, y1, x2, y2), bbox_width, bbox_height, bbox_ratio, bbox_class\n",
    "\n",
    "def calculate_head_upper_body_speed(keypoints, prev_keypoints):\n",
    "    h = np.array([keypoints[0, 0], keypoints[0, 1]])  # 머리 좌표\n",
    "    l = np.array([keypoints[11, 0], keypoints[11, 1]])  # 왼쪽 어깨 좌표\n",
    "    r = np.array([keypoints[12, 0], keypoints[12, 1]])  # 오른쪽 어깨 좌표\n",
    "\n",
    "    # 이전 프레임의 좌표\n",
    "    prev_h = np.array([prev_keypoints[0, 0], prev_keypoints[0, 1]])\n",
    "    prev_l = np.array([prev_keypoints[11, 0], prev_keypoints[11, 1]])\n",
    "    prev_r = np.array([prev_keypoints[12, 0], prev_keypoints[12, 1]])\n",
    "\n",
    "    # 현재 프레임과 이전 프레임의 상체 중심\n",
    "    center_new = (h + l + r) / 3\n",
    "    center_prev = (prev_h + prev_l + prev_r) / 3\n",
    "\n",
    "    # 유클리드 거리 계산 (속도)\n",
    "    speed = distance.euclidean(center_new, center_prev)\n",
    "    return speed\n",
    "\n",
    "# MediaPipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "def process_landmarks(landmarks): \n",
    "    selected_landmarks = landmarks[LANDMARKS]\n",
    "    return selected_landmarks[:, :2].flatten()\n",
    "\n",
    "# GRU 모델 정의\n",
    "class GRUModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=3, dropout=0.5):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# GRU 모델 로드\n",
    "input_size = 27  \n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 3\n",
    "dropout = 0.5    \n",
    "\n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "gru_model.load_state_dict(torch.load('D:\\\\project\\\\prjvenv\\\\GRU\\\\best_GRU_model.pt', map_location=torch.device('cpu')))\n",
    "gru_model.eval()\n",
    "\n",
    "# 클래스 이름 정의\n",
    "class_names = {0: 'Normal', 1: 'Fall', 2: 'Danger'}\n",
    "\n",
    "# 낙상 감지 함수\n",
    "def detect_fall(landmarks, prev_landmarks, frame):\n",
    "    speed = calculate_head_upper_body_speed(landmarks, prev_landmarks) if prev_landmarks is not None else 0\n",
    "    processed_landmarks = process_landmarks(landmarks)\n",
    "    \n",
    "    bbox_info = calculate_and_draw_bbox(frame, landmarks)\n",
    "    \n",
    "    if bbox_info is None:\n",
    "        return None\n",
    "    \n",
    "    bbox_width, bbox_height, bbox_ratio, bbox_class = bbox_info[1:4] + (bbox_info[4],)\n",
    "    \n",
    "    print(f\"Processed landmarks length: {len(processed_landmarks)}\")\n",
    "    print(f\"BBox width: {bbox_width}, height: {bbox_height}, ratio: {bbox_ratio}, speed: {speed}\")   \n",
    "\n",
    "    # processed_landmarks와 함께 바운딩 박스 좌표 및 속도 정보 추가\n",
    "    input_data = np.concatenate([processed_landmarks,\n",
    "                                  [bbox_width,\n",
    "                                   bbox_height,\n",
    "                                   bbox_ratio,\n",
    "                                   speed,\n",
    "                                   bbox_class]])\n",
    "    \n",
    "    if len(input_data) != input_size:\n",
    "        print(f\"Warning: input_data length is {len(input_data)}, expected {input_size}\")\n",
    "        return None\n",
    "    \n",
    "    input_tensor = torch.FloatTensor(input_data).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = gru_model(input_tensor)\n",
    "\n",
    "    probabilities = torch.softmax(output, dim=1).numpy()[0]\n",
    "    predicted_class = torch.argmax(output).item()\n",
    "    \n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# 비디오 파일 경로 지정\n",
    "video_path = \"D:\\\\human_fall\\\\re_video\\\\validation\\\\N\\\\02327_H_A_N_C6.mp4\"\n",
    "# 비디오 파일 열기\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 비디오 속성 가져오기\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# 출력 비디오 설정\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_path='data_video_test_outputY_GRU_only.mp4'\n",
    "out= cv2.VideoWriter(out_path,fourcc,fps,(width,height))\n",
    "\n",
    "prev_landmarks=None\n",
    "\n",
    "# 프레임 처리 루프\n",
    "while cap.isOpened():\n",
    "    ret , frame= cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # MediaPipe로 포즈 추정 \n",
    "    rgb_frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results_pose=pose.process(rgb_frame)\n",
    "\n",
    "    if results_pose.pose_landmarks:\n",
    "       landmarks=np.array([[lm.x * width , lm.y * height , lm.z] for lm in results_pose.pose_landmarks.landmark])\n",
    "       \n",
    "       # 바운딩 박스 계산 및 그리기 \n",
    "       bbox_info=calculate_and_draw_bbox(frame , landmarks)\n",
    "\n",
    "       if prev_landmarks is not None: \n",
    "           label , probs=detect_fall(landmarks , prev_landmarks , frame)\n",
    "           print(f\"Predicted Class: {label}, Probabilities: {probs}\")  \n",
    "       else: \n",
    "           label=None \n",
    "\n",
    "       prev_landmarks=landmarks \n",
    "\n",
    "       # 바운딩 박스와 라벨 그리기 \n",
    "       x1 , y1 , x2 , y2=bbox_info[0]   \n",
    "       color=(0 ,255 ,0) if label==0 else ((0, 255, 255) if label==1 else (255, 0, 0)) \n",
    "       cv2.rectangle(frame , (x1 , y1) , (x2 , y2) , color ,2)\n",
    "       # 클래스 이름을 사용하여 텍스트 표시\n",
    "       class_name = class_names[label] if label is not None else 'Unknown'\n",
    "       cv2.putText(frame , f'GRU: {label}' , (x1 , y1 -10) , cv2.FONT_HERSHEY_SIMPLEX ,0.7 , color ,2)\n",
    "\n",
    "       # 랜드마크 표시 \n",
    "       mp_drawing.draw_landmarks(frame , results_pose.pose_landmarks , mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # 프레임 저장 및 출력 \n",
    "    resized_frame=cv2.resize(frame,(1920 ,1080))  \n",
    "    out.write(resized_frame) \n",
    "    cv2.imshow('Fall Detection' , resized_frame) \n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "       break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU 모델만 사용\n",
    "* 속도를 우선적으로 클래스 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 2\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 2\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 2\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 0\n",
      "Predicted Class: 1\n",
      "Predicted Class: 0\n",
      "Predicted Class: 1\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# MediaPipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 랜드마크 인덱스 정의 (예: 코, 왼쪽 어깨, 오른쪽 어깨 등)\n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]  # 총 11개 랜드마크\n",
    "\n",
    "# GRU 모델 정의\n",
    "class GRUModel(torch.nn.Module):\n",
    "    def __init__(self, input_size=27):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size = 64\n",
    "        self.num_layers = num_layers = 2\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,\n",
    "                          dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, 3)  # output_size를 직접 지정합니다.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# GRU 모델 로드\n",
    "input_size = 27\n",
    "gru_model = GRUModel(input_size=input_size)  \n",
    "gru_model.load_state_dict(torch.load('D:\\\\project\\\\prjvenv\\\\GRU\\\\best_GRU_model_2.pt', map_location=torch.device('cpu')))\n",
    "gru_model.eval()\n",
    "\n",
    "# 클래스 이름 정의\n",
    "class_names = {0: 'Normal', 1: 'Fall', 2: 'Danger'}\n",
    "\n",
    "# Threshold 값 정의\n",
    "threshold_normal = 6.5   # 일반 상태로 간주되는 속도 임계값\n",
    "threshold_danger = 10.5   # 위험 상태로 간주되는 속도 임계값\n",
    "\n",
    "def calculate_head_upper_body_speed(keypoints, prev_keypoints):\n",
    "    h = np.array([keypoints[0, 0], keypoints[0, 1]])   # 머리 좌표\n",
    "    l = np.array([keypoints[11, 0], keypoints[11, 1]])  # 왼쪽 어깨 좌표\n",
    "    r = np.array([keypoints[12, 0], keypoints[12, 1]])  # 오른쪽 어깨 좌표\n",
    "\n",
    "    # 이전 프레임의 좌표가 없는 경우 속도는 0으로 설정\n",
    "    if prev_keypoints is None:\n",
    "        return 0.0\n",
    "\n",
    "    prev_h = np.array([prev_keypoints[0, 0], prev_keypoints[0, 1]])\n",
    "    prev_l = np.array([prev_keypoints[11, 0], prev_keypoints[11, 1]])\n",
    "    prev_r = np.array([prev_keypoints[12, 0], prev_keypoints[12, 1]])\n",
    "\n",
    "    # 현재 프레임과 이전 프레임의 상체 중심 계산\n",
    "    center_new = (h + l + r) / 3\n",
    "    center_prev = (prev_h + prev_l + prev_r) / 3\n",
    "\n",
    "    # 유클리드 거리 계산 (속도)\n",
    "    speed = distance.euclidean(center_new, center_prev)\n",
    "    return speed\n",
    "\n",
    "def process_landmarks(landmarks): \n",
    "    selected_landmarks = landmarks[LANDMARKS]   # 지정된 랜드마크 선택 \n",
    "    return selected_landmarks[:, :2].flatten()   # (x,y) 좌표 반환\n",
    "\n",
    "def calculate_and_draw_bbox(frame, landmarks):\n",
    "    x_coordinates = landmarks[:, 0]\n",
    "    y_coordinates = landmarks[:, 1]\n",
    "    \n",
    "    x1 = max(0, int(np.min(x_coordinates)))\n",
    "    y1 = max(0, int(np.min(y_coordinates)))\n",
    "    x2 = min(frame.shape[1], int(np.max(x_coordinates)))\n",
    "    y2 = min(frame.shape[0], int(np.max(y_coordinates)))\n",
    "    \n",
    "    bbox_width = x2 - x1\n",
    "    bbox_height = y2 - y1\n",
    "    \n",
    "    # 높이가 0일 경우 비율을 무한대로 설정\n",
    "    bbox_ratio = bbox_width / bbox_height if bbox_height != 0 else float('inf')\n",
    "    \n",
    "    # 바운딩 박스를 조금 더 넓게 조정 (각 방향으로 패딩 추가)\n",
    "    padding = 50\n",
    "    x1 = max(0, x1 - padding)\n",
    "    y1 = max(0, y1 - padding)\n",
    "    x2 = min(frame.shape[1], x2 + padding)\n",
    "    y2 = min(frame.shape[0], y2 + padding)\n",
    "\n",
    "    return (x1, y1), (x2, y2), bbox_width, bbox_height\n",
    "\n",
    "# 낙상 감지 함수\n",
    "def detect_fall(landmarks, prev_landmarks):\n",
    "    speed = calculate_head_upper_body_speed(landmarks, prev_landmarks)\n",
    "    \n",
    "    processed_landmarks = process_landmarks(landmarks)\n",
    "\n",
    "    # 바운딩 박스 계산 및 그리기 \n",
    "    top_left_bbox , bottom_right_bbox , bbox_width , bbox_height= calculate_and_draw_bbox(frame , landmarks)\n",
    "\n",
    "    \n",
    "    # 속도 기반 클래스 결정\n",
    "    if speed < threshold_normal:\n",
    "        bbox_class = 0   # Normal \n",
    "    elif speed < threshold_danger:\n",
    "        bbox_class = 2   # Danger \n",
    "    else:\n",
    "        bbox_class = 1   # Fall \n",
    "\n",
    "    return bbox_class\n",
    "\n",
    "# 비디오 파일 경로 지정 및 열기 \n",
    "video_path=\"D:\\\\human_fall\\\\re_video\\\\validation\\\\Y\\\\00170_H_A_SY_C5.mp4\"\n",
    "cap=cv2.VideoCapture(video_path)\n",
    "\n",
    "# 비디오 속성 가져오기 \n",
    "width=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps=cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# 출력 비디오 설정 \n",
    "fourcc=cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_path='video_test_GRU_onlyspeed_8.mp4'\n",
    "out=cv2.VideoWriter(out_path,fourcc,fps,(width,height))\n",
    "\n",
    "prev_landmarks=None\n",
    "\n",
    "# 프레임 처리 루프 \n",
    "while cap.isOpened():\n",
    "    ret , frame=cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame=cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n",
    "    results_pose=pose.process(rgb_frame)\n",
    "\n",
    "    if results_pose.pose_landmarks:\n",
    "        landmarks=np.array([[lm.x * width , lm.y * height , lm.z] for lm in results_pose.pose_landmarks.landmark])\n",
    "       \n",
    "        if prev_landmarks is not None: \n",
    "            label=detect_fall(landmarks , prev_landmarks)  \n",
    "            print(f\"Predicted Class: {label}\")  \n",
    "        else: \n",
    "            label=None \n",
    "\n",
    "        prev_landmarks=landmarks \n",
    "\n",
    "        # 바운딩 박스와 라벨 그리기 \n",
    "        top_left_bbox , bottom_right_bbox , _, _= calculate_and_draw_bbox(frame , landmarks)  \n",
    "        color=(0 ,255 ,0) if label==0 else ((0 ,255, 255) if label==2 else (0, 0, 255)) \n",
    "        cv2.rectangle(frame , top_left_bbox , bottom_right_bbox , color ,2)\n",
    "        class_name=class_names[label] if label is not None else 'Unknown'\n",
    "        cv2.putText(frame , f'GRU: {class_name}' , (top_left_bbox[0] , top_left_bbox[1] -10) , cv2.FONT_HERSHEY_SIMPLEX ,0.7 , color ,2)\n",
    "\n",
    "        # 랜드마크 표시 \n",
    "        mp_drawing.draw_landmarks(frame , results_pose.pose_landmarks , mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # 프레임 저장 및 출력 \n",
    "    resized_frame=cv2.resize(frame,(1920 ,1080))  \n",
    "    out.write(frame) \n",
    "    cv2.imshow('Fall Detection' , resized_frame) \n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "         break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU 모델만 사용\n",
    "* 속도 기반으로 초기 클래스 결정 후 bbox의 비율로 클래스 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 1\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 0\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n",
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "# MediaPipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 랜드마크 인덱스 정의 (예: 코, 왼쪽 어깨, 오른쪽 어깨 등)\n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]  # 총 11개 랜드마크\n",
    "\n",
    "# GRU 모델 정의\n",
    "class GRUModel(torch.nn.Module):\n",
    "    def __init__(self, input_size=27):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size = 64\n",
    "        self.num_layers = num_layers = 2\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,\n",
    "                          dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, 3)  # output_size를 직접 지정합니다.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# GRU 모델 로드\n",
    "input_size = 27\n",
    "gru_model = GRUModel(input_size=input_size)  \n",
    "gru_model.load_state_dict(torch.load('D:\\\\project\\\\prjvenv\\\\GRU\\\\best_GRU_model_2.pt', map_location=torch.device('cpu')))\n",
    "gru_model.eval()\n",
    "\n",
    "# 클래스 이름 정의\n",
    "class_names = {0: 'Normal', 1: 'Fall', 2: 'Danger'}\n",
    "\n",
    "# Threshold 값 정의\n",
    "threshold_normal = 6.5   # 일반 상태로 간주되는 속도 임계값\n",
    "threshold_danger = 10.5   # 위험 상태로 간주되는 속도 임계값\n",
    "\n",
    "def calculate_head_upper_body_speed(keypoints, prev_keypoints):\n",
    "    h = np.array([keypoints[0, 0], keypoints[0, 1]])   # 머리 좌표\n",
    "    l = np.array([keypoints[11, 0], keypoints[11, 1]])  # 왼쪽 어깨 좌표\n",
    "    r = np.array([keypoints[12, 0], keypoints[12, 1]])  # 오른쪽 어깨 좌표\n",
    "\n",
    "    # 이전 프레임의 좌표가 없는 경우 속도는 0으로 설정\n",
    "    if prev_keypoints is None:\n",
    "        return 0.0\n",
    "\n",
    "    prev_h = np.array([prev_keypoints[0, 0], prev_keypoints[0, 1]])\n",
    "    prev_l = np.array([prev_keypoints[11, 0], prev_keypoints[11, 1]])\n",
    "    prev_r = np.array([prev_keypoints[12, 0], prev_keypoints[12, 1]])\n",
    "\n",
    "    # 현재 프레임과 이전 프레임의 상체 중심 계산\n",
    "    center_new = (h + l + r) / 3\n",
    "    center_prev = (prev_h + prev_l + prev_r) / 3\n",
    "\n",
    "    # 유클리드 거리 계산 (속도)\n",
    "    speed = distance.euclidean(center_new, center_prev)\n",
    "    return speed\n",
    "\n",
    "def process_landmarks(landmarks): \n",
    "    selected_landmarks = landmarks[LANDMARKS]   # 지정된 랜드마크 선택 \n",
    "    return selected_landmarks[:, :2].flatten()   # (x,y) 좌표 반환\n",
    "\n",
    "def calculate_and_draw_bbox(frame, landmarks):\n",
    "    x_coordinates = landmarks[:, 0]\n",
    "    y_coordinates = landmarks[:, 1]\n",
    "    \n",
    "    x1 = max(0, int(np.min(x_coordinates)))\n",
    "    y1 = max(0, int(np.min(y_coordinates)))\n",
    "    x2 = min(frame.shape[1], int(np.max(x_coordinates)))\n",
    "    y2 = min(frame.shape[0], int(np.max(y_coordinates)))\n",
    "    \n",
    "    bbox_width = x2 - x1\n",
    "    bbox_height = y2 - y1\n",
    "    \n",
    "    # 높이가 0일 경우 비율을 무한대로 설정\n",
    "    bbox_ratio = bbox_width / bbox_height if bbox_height != 0 else float('inf')\n",
    "    \n",
    "    # 바운딩 박스를 조금 더 넓게 조정 (각 방향으로 패딩 추가)\n",
    "    padding = 50\n",
    "    x1 = max(0, x1 - padding)\n",
    "    y1 = max(0, y1 - padding)\n",
    "    x2 = min(frame.shape[1], x2 + padding)\n",
    "    y2 = min(frame.shape[0], y2 + padding)\n",
    "\n",
    "    return (x1, y1), (x2, y2), bbox_width, bbox_height\n",
    "\n",
    "# 낙상 감지 함수\n",
    "def detect_fall(landmarks, prev_landmarks):\n",
    "    speed = calculate_head_upper_body_speed(landmarks, prev_landmarks)\n",
    "    \n",
    "    processed_landmarks = process_landmarks(landmarks)\n",
    "\n",
    "    # 바운딩 박스 계산 및 그리기 \n",
    "    top_left_bbox , bottom_right_bbox , bbox_width , bbox_height= calculate_and_draw_bbox(frame , landmarks)\n",
    "    \n",
    "    # 바운딩 박스 비율 계산\n",
    "    bbox_ratio = bbox_width / bbox_height if bbox_height != 0 else float('inf')\n",
    "\n",
    "    \n",
    "    # 속도 기반 클래스 결정\n",
    "    if speed < threshold_normal:\n",
    "        bbox_class = 0   # Normal \n",
    "    elif speed < threshold_danger:\n",
    "        bbox_class = 2   # Danger \n",
    "    else:\n",
    "        bbox_class = 1   # Fall \n",
    "    \n",
    "    # 바운딩 박스 비율에 따른 클래스 조정\n",
    "    if bbox_class == 0 and bbox_ratio < 0.5 :\n",
    "        bbox_class = 0   # Normal에서 Normal로 조정\n",
    "    elif bbox_class == 0 and 0.5 <= bbox_ratio <= 0.7 : \n",
    "        bbox_class = 2   # Normal에서 Danger로 조정\n",
    "    elif bbox_class == 0 and bbox_ratio > 1 : \n",
    "        bbox_class = 1   # Normal에서 Fall로 조정\n",
    "        \n",
    "    elif bbox_class == 2 and bbox_ratio < 0.5 : \n",
    "        bbox_class = 0\n",
    "    elif bbox_class == 2 and 0.5 <= bbox_ratio <= 0.7 : \n",
    "        bbox_class = 2\n",
    "    elif bbox_class == 2 and bbox_ratio > 1 : \n",
    "        bbox_class = 1\n",
    "        \n",
    "    elif bbox_class == 1 and bbox_ratio < 0.5 : \n",
    "        bbox_class = 0\n",
    "    elif bbox_class == 1 and 0.5 <= bbox_ratio <= 0.7 : \n",
    "        bbox_class = 2\n",
    "    elif bbox_class == 1 and bbox_ratio > 1 : \n",
    "        bbox_class = 1\n",
    "\n",
    "    return bbox_class\n",
    "\n",
    "# 비디오 파일 경로 지정 및 열기 \n",
    "video_path=\"D:\\\\human_fall\\\\re_video\\\\training\\\\N\\\\01745_Y_E_N_C1.mp4\"\n",
    "cap=cv2.VideoCapture(video_path)\n",
    "\n",
    "# 비디오 속성 가져오기 \n",
    "width=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps=cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# 출력 비디오 설정 \n",
    "fourcc=cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_path='GRU_speed+bboxratio_N_8.mp4'\n",
    "out=cv2.VideoWriter(out_path,fourcc,fps,(width,height))\n",
    "\n",
    "prev_landmarks=None\n",
    "\n",
    "# 프레임 처리 루프 \n",
    "while cap.isOpened():\n",
    "    ret , frame=cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame=cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n",
    "    results_pose=pose.process(rgb_frame)\n",
    "\n",
    "    if results_pose.pose_landmarks:\n",
    "        landmarks=np.array([[lm.x * width , lm.y * height , lm.z] for lm in results_pose.pose_landmarks.landmark])\n",
    "       \n",
    "        if prev_landmarks is not None: \n",
    "            label=detect_fall(landmarks , prev_landmarks)  \n",
    "            print(f\"Predicted Class: {label}\")  \n",
    "        else: \n",
    "            label=None \n",
    "\n",
    "        prev_landmarks=landmarks \n",
    "\n",
    "        # 바운딩 박스와 라벨 그리기 \n",
    "        top_left_bbox , bottom_right_bbox , _, _= calculate_and_draw_bbox(frame , landmarks)  \n",
    "        color=(0 ,255 ,0) if label==0 else ((0 ,255, 255) if label==2 else (0, 0, 255)) \n",
    "        cv2.rectangle(frame , top_left_bbox , bottom_right_bbox , color ,2)\n",
    "        class_name=class_names[label] if label is not None else 'Unknown'\n",
    "        cv2.putText(frame , f'GRU: {class_name}' , (top_left_bbox[0] , top_left_bbox[1] -10) , cv2.FONT_HERSHEY_SIMPLEX ,0.7 , color ,2)\n",
    "\n",
    "        # 랜드마크 표시 \n",
    "        mp_drawing.draw_landmarks(frame , results_pose.pose_landmarks , mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # 프레임 저장 및 출력 \n",
    "    resized_frame=cv2.resize(frame,(1920 ,1080))  \n",
    "    out.write(frame) \n",
    "    cv2.imshow('Fall Detection' , resized_frame) \n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "         break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Fall, 74.9ms\n",
      "Speed: 2.0ms preprocess, 74.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 2.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 2.0ms\n",
      "Speed: 2.0ms preprocess, 2.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 2.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 3.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 4.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 5.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 6.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 6.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 5.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 Fall, 21.0ms\n",
      "Speed: 3.0ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 19.0ms\n",
      "Speed: 5.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 2.0ms preprocess, 24.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 1.0ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 6.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 2.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 4.0ms preprocess, 24.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 5.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 3.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 3.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 5.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 6.0ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 5.0ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 5.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 3.0ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 5.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 5.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 3.0ms preprocess, 23.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 4.0ms preprocess, 23.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 2.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 4.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 2.0ms preprocess, 23.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 6.0ms preprocess, 23.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 6.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 6.0ms preprocess, 22.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 2.0ms preprocess, 23.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 3.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 3.0ms preprocess, 27.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 3.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 3.0ms preprocess, 32.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 3.0ms preprocess, 32.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 30.0ms\n",
      "Speed: 3.0ms preprocess, 30.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 2.0ms preprocess, 27.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 4.0ms preprocess, 24.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 29.0ms\n",
      "Speed: 3.0ms preprocess, 29.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 3.0ms preprocess, 24.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 1.0ms preprocess, 26.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 2.0ms preprocess, 24.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 3.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 2.0ms preprocess, 24.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 3.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 36.0ms\n",
      "Speed: 2.0ms preprocess, 36.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 24.0ms\n",
      "Speed: 2.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 37.0ms\n",
      "Speed: 2.0ms preprocess, 37.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 38.0ms\n",
      "Speed: 6.0ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 44.0ms\n",
      "Speed: 1.0ms preprocess, 44.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 60.0ms\n",
      "Speed: 2.0ms preprocess, 60.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 3.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 33.0ms\n",
      "Speed: 3.0ms preprocess, 33.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 48.0ms\n",
      "Speed: 3.0ms preprocess, 48.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 36.0ms\n",
      "Speed: 4.0ms preprocess, 36.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 4.0ms preprocess, 32.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 39.0ms\n",
      "Speed: 3.0ms preprocess, 39.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 36.0ms\n",
      "Speed: 4.0ms preprocess, 36.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 4.0ms preprocess, 32.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 4.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 38.0ms\n",
      "Speed: 4.0ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 4.0ms preprocess, 32.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 5.0ms preprocess, 28.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 50.0ms\n",
      "Speed: 2.0ms preprocess, 50.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 5.0ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 38.0ms\n",
      "Speed: 3.0ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 3.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 4.0ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 33.0ms\n",
      "Speed: 3.0ms preprocess, 33.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 4.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 35.0ms\n",
      "Speed: 4.0ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 3.0ms preprocess, 32.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 4.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 2.0ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 5.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 34.0ms\n",
      "Speed: 3.0ms preprocess, 34.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 3.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 6.0ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 3.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 4.0ms preprocess, 32.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 5.0ms preprocess, 31.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 4.0ms preprocess, 27.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 3.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 2.0ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 43.0ms\n",
      "Speed: 2.0ms preprocess, 43.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 40.0ms\n",
      "Speed: 3.0ms preprocess, 40.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 3.0ms preprocess, 32.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 38.0ms\n",
      "Speed: 2.0ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 36.0ms\n",
      "Speed: 2.0ms preprocess, 36.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 37.0ms\n",
      "Speed: 2.0ms preprocess, 37.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 4.0ms preprocess, 32.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 37.0ms\n",
      "Speed: 3.0ms preprocess, 37.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 34.0ms\n",
      "Speed: 2.0ms preprocess, 34.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 9.0ms preprocess, 32.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 34.0ms\n",
      "Speed: 5.0ms preprocess, 34.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 5.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 4.0ms preprocess, 27.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 5.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 30.0ms\n",
      "Speed: 3.0ms preprocess, 30.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 33.0ms\n",
      "Speed: 2.0ms preprocess, 33.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 34.0ms\n",
      "Speed: 10.0ms preprocess, 34.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 33.0ms\n",
      "Speed: 2.0ms preprocess, 33.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 35.0ms\n",
      "Speed: 8.0ms preprocess, 35.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 5.0ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 36.0ms\n",
      "Speed: 1.0ms preprocess, 36.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 5.0ms preprocess, 32.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 47.0ms\n",
      "Speed: 5.0ms preprocess, 47.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 34.0ms\n",
      "Speed: 2.0ms preprocess, 34.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 35.0ms\n",
      "Speed: 5.0ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 44.0ms\n",
      "Speed: 3.0ms preprocess, 44.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 33.0ms\n",
      "Speed: 3.0ms preprocess, 33.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 36.0ms\n",
      "Speed: 5.0ms preprocess, 36.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 36.0ms\n",
      "Speed: 3.0ms preprocess, 36.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 4.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 1.0ms preprocess, 18.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 3.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 3.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 4.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 1.0ms preprocess, 18.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 2.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 3.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 2.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 3.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 3.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 4.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 14.0ms\n",
      "Speed: 1.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 29.0ms\n",
      "Speed: 2.0ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 3.0ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 45.0ms\n",
      "Speed: 4.0ms preprocess, 45.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 4.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 30.0ms\n",
      "Speed: 5.0ms preprocess, 30.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 3.0ms preprocess, 28.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 39.0ms\n",
      "Speed: 2.0ms preprocess, 39.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 38.0ms\n",
      "Speed: 2.0ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 3.0ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 5.0ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 23.0ms\n",
      "Speed: 3.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 5.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 38.0ms\n",
      "Speed: 3.0ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 4.0ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 27.0ms\n",
      "Speed: 3.0ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 33.0ms\n",
      "Speed: 2.0ms preprocess, 33.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 3.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 5.0ms preprocess, 32.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 25.0ms\n",
      "Speed: 3.0ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 35.0ms\n",
      "Speed: 3.0ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 30.0ms\n",
      "Speed: 3.0ms preprocess, 30.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 26.0ms\n",
      "Speed: 5.0ms preprocess, 26.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 32.0ms\n",
      "Speed: 3.0ms preprocess, 32.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n",
      "\n",
      "0: 384x640 1 Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "GRU 모델 실행 중 에러: input.size(-1) must be equal to input_size. Expected 27, got 26\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "# YOLO 모델 로드\n",
    "yolo_model = YOLO('D:\\\\project\\\\prjvenv\\\\runs\\\\detect\\\\human_fall_s30\\\\weights\\\\best.pt')\n",
    "\n",
    "# 낙상 감지 함수\n",
    "def detect_fall(landmarks, bbox_width, bbox_height, bbox_ratio, confidence):\n",
    "    processed_landmarks = process_landmarks(landmarks)\n",
    "    \n",
    "    # 랜드마크의 수를 확인\n",
    "    if processed_landmarks.shape[0] != 22:  # 11개 랜드마크 * 2 (x, y)\n",
    "        print(f\"Processed landmarks count: {processed_landmarks.shape[0]}, expected 22\")\n",
    "    \n",
    "    # 입력 데이터 생성 (confidence 포함)\n",
    "    input_data = np.concatenate([processed_landmarks, [bbox_width, bbox_height, bbox_ratio, confidence]])\n",
    "    \n",
    "    # 입력 데이터 크기 확인\n",
    "    if len(input_data) != 26:\n",
    "        print(f\"input_data length: {len(input_data)}, expected 26\")\n",
    "        return None, None\n",
    "    \n",
    "    input_tensor = torch.FloatTensor(input_data).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = gru_model(input_tensor)\n",
    "    \n",
    "    probabilities = torch.softmax(output, dim=1).numpy()[0]\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    print(f\"Probabilities: Normal={probabilities[0]:.4f}, Danger={probabilities[1]:.4f}, Fall={probabilities[2]:.4f}\")\n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# MediaPipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "def process_landmarks(landmarks): \n",
    "    selected_landmarks = landmarks[LANDMARKS]\n",
    "    return selected_landmarks[:, :2].flatten()\n",
    "\n",
    "# GRU 모델 로드\n",
    "class GRUModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=3, dropout=0.5):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "input_size = 27 # 랜드마크 x,y 좌표 + 바운딩박스 비율 + confidence\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 3\n",
    "dropout = 0.5    \n",
    "\n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "gru_model.load_state_dict(torch.load('D:\\\\project\\\\prjvenv\\\\GRU\\\\best_GRU_model_2.pt', map_location=torch.device('cpu')))\n",
    "gru_model.eval()\n",
    "\n",
    "# 비디오 파일 경로 지정\n",
    "video_path = \"D:\\\\human_fall\\\\re_video\\\\training\\\\Y\\\\02735_H_A_FY_C7.mp4\"\n",
    "\n",
    "# 비디오 파일 열기\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 비디오 속성 가져오기\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "window_w = int(width * 0.3)\n",
    "window_h = int(height * 0.3)\n",
    "\n",
    "cv2.namedWindow('Fall Detection', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Fall Detection', window_w, window_h)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# 출력 비디오 설정\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('data_video_test_outputY.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "confidence_threshold = 0.3\n",
    "\n",
    "previous_bbox = None\n",
    "previous_label = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLO 모델 실행\n",
    "    results = yolo_model(frame)\n",
    "    \n",
    "    # YOLO 결과 처리\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "        confidences = result.boxes.conf.cpu().numpy()\n",
    "        class_ids = result.boxes.cls.cpu().numpy()\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            # YOLO가 객체를 감지한 경우 (바운딩 박스만 사용)\n",
    "            box_indices_to_process = [i for i in range(len(boxes)) if confidences[i] > confidence_threshold]\n",
    "            for i in box_indices_to_process:\n",
    "                box = boxes[i]  # 각 객체의 바운딩 박스 처리\n",
    "                confidence = confidences[i]\n",
    "                class_id = class_ids[i]\n",
    "                \n",
    "                x1, y1, x2, y2 = box\n",
    "                bbox_width = x2 - x1\n",
    "                bbox_height = y2 - y1\n",
    "                bbox_ratio = bbox_width / bbox_height\n",
    "                \n",
    "                # MediaPipe 처리\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results_pose = pose.process(rgb_frame)\n",
    "                \n",
    "                if results_pose.pose_landmarks:\n",
    "                    landmarks = np.array([[lm.x * frame.shape[1], lm.y * frame.shape[0]] for lm in results_pose.pose_landmarks.landmark])\n",
    "                    \n",
    "                    try:\n",
    "                        # GRU 모델을 통해 클래스 예측 (confidence를 포함하여 호출)\n",
    "                        label, probs = detect_fall(landmarks, bbox_width, bbox_height, bbox_ratio ,confidence) \n",
    "                        previous_bbox = (x1,y1,x2,y2)\n",
    "                        previous_label = label\n",
    "\n",
    "                        # 바운딩 박스 색상 결정 (확률 기반으로 결정)\n",
    "                        if probs[2] > 0.5:  # Fall 클래스 확률이 50% 이상일 때 빨간색으로 표시 (Fall은 빨간색으로 설정됨)\n",
    "                            color = (0 ,0 ,255)   # Fall: Red\n",
    "                        elif probs[1] > 0.5:   # Danger 클래스 확률이 50% 이상일 때 노란색으로 표시\n",
    "                            color = (0 ,255 ,255)   # Danger: Yellow\n",
    "                        else:\n",
    "                            color = (0 ,255 ,0)   # Normal: Green\n",
    "\n",
    "                        # 바운딩 박스 그리기 및 레이블 표시하기\n",
    "                        cv2.rectangle(frame,(x1,y1),(x2,y2),color ,2) \n",
    "                        cv2.putText(frame,f'Class: {label}', (x1,y1 -10), cv2.FONT_HERSHEY_SIMPLEX ,0.7,color ,2)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"GRU 모델 실행 중 에러: {e}\")\n",
    "                        label, probs = previous_label if previous_label is not None else (None,None)\n",
    "\n",
    "                else:\n",
    "                    # MediaPipe가 랜드마크를 감지하지 못한 경우 YOLO 결과 표시 (바운딩 박스만 그리기)\n",
    "                    color_map_default = {0: (255 ,0 ,0), 1: (255 ,255 ,0)}  \n",
    "                    color_default= color_map_default[class_id] if class_id in color_map_default else (255 ,255 ,255) \n",
    "                    cv2.rectangle(frame,(x1,y1),(x2,y2),color_default ,2)  \n",
    "                    cv2.putText(frame,f\"YOLO Class ID: {class_id}\",(x1,y1 -30),cv2.FONT_HERSHEY_SIMPLEX ,0.7,color_default ,2)\n",
    "\n",
    "    # 프레임 저장 및 출력\n",
    "    out.write(frame)\n",
    "    cv2.imshow('Fall Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
